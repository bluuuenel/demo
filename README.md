# demo
1 导言
梯度下降法是机器学习中一种常用到的算法，但其本身不是机器学习算法，而是一种求解的最优化算法。主要解决求最小值问题，其基本思想在于不断地逼近最优点，每一步的优化方向就是梯度的方向。
机器学习的本质就是“喂”给模型数据，让模型不断地去学习，而这个学习的过程就是利用梯度下降法不断去优化的过程，目前最为常见的深度神经网络便是利用梯度的反向传播，反复更新模型参数直至收敛，从而达到优化模型的目的。
对于最简单的线性模型，如

我们假设其损失函数为

那么梯度下降的基本形式就是


其中，
为学习率（learning_rate）。下一步便是要将损失函数最小化，需要对
求导：

其中
，所以不难得出


即

常见的梯度下降法包括随机梯度下降法（SGD）、批梯度下降法、Momentum梯度下降法、Nesterov Momentum梯度下降法、AdaGrad梯度下降法、RMSprop梯度下降法、Adam梯度下降法。下面分别对各种梯度下降法进行介绍。

梯度下降法」 的目标是搜索出来一个能够让函数的值尽可能小的位置，所以应该让 
 朝着黑色箭头的方向走，那么怎么完成这个操作呢？接下来的过程我使用伪代码来表示。

在代码中有一个 
 变量，它的专业名词叫 「学习率」。使用数学表达式来表示更新 
 的过程那就是：


表达式的意思就是让 
 减去 
 乘以函数的导数。其中的 
 是为了控制 
 更新的幅度，将 
 设置的小一点，那么每一次更新的幅度就会小一点。这是有用的，在后边儿我会解释 
 的设置会出现什么样的问题。

「初始化：」

# 变量 x 表示当前所在的位置
# 就随机初始在 6 这个位置好了
x = 6
eta = 0.05
「第 1 次更新 x」

# 变量 df 存储当前位置的导数值
df = 2*x-2
x = x - eta*df
# 更新后 x 由 6 变成 
# 6-0.05*10 = 5.5
「第 2 次更新 x」

# 因为 x 的位置发生了变化
# 要重新计算当前位置的导数
df = 2*x-2
x = x - eta*df
# 更新后 x 由 5.5 变成
# 5.5-0.05*9 = 5.05
好啦，接下来只要重复刚才的过程就可以了。那么，把刚才的两个步骤画出来。其中红色箭头指向的点是初始时候的点，「黄色」 箭头指向的点是第 1 次更新后的位置，「蓝色」 箭头指向的点是第 2 次更新后的位置。


只要不停地重复刚才的这个过程，那么最终就会收敛到一个 「局部」 的极值点。那么可以想一下这是为什么？因为随着每一次的更新，曲线都会越来越平缓，相应的导数值也会越来越小，当我们接近极值点的时候导数的值会无限地靠近 「0」。由于导数的绝对值越来越小，那么随后更新的幅度也会越来越小，最终就会停留在极值点的位置了。接下来我将用 Python 来实现这个过程，并让刚才的步骤迭代 1000 次。

「算法初始化」

import matplotlib.pyplot as plt
import numpy as np

# 初始算法开始之前的坐标 
# cur_x 和 cur_y 
cur_x = 6
cur_y = (cur_x-1)**2 + 1
# 设置学习率 eta 为 0.05
eta = 0.05
# 变量 iter 用于存储迭代次数
# 这次我们迭代 1000 次
# 所以给它赋值 1000
iter = 1000
# 变量 cur_df 用于存储
# 当前位置的导数
# 一开始我给它赋值为 None
# 每一轮循环的时候为它更新值
cur_df = None

# all_x 用于存储
# 算法进行时所有点的横坐标
all_x = []
# all_y 用于存储
# 算法进行时所有点的纵坐标
all_y = []

# 把最一开始的坐标存储到
# all_x 和 all_y 中
all_x.append(cur_x)
all_y.append(cur_y)
「迭代 1000 次」

# 循环结束也就意味着算法的结束
for i in range(iter):
    # 每一次迭代之前先计算
    # 当前位置的梯度 cur_df
    # cur 是英文单词 current 
    cur_df = 2*cur_x - 2
    # 更新 cur_x 到下一个位置
    cur_x = cur_x - eta*cur_df
    # 更新下一个 cur_x 对应的 cur_y
    cur_y = (cur_x-1)**2 + 1

    # 其实 cur_y 并没有起到实际的计算作用
    # 在这里计算 cur_y 只是为了将每一次的
    # 点的坐标存储到 all_x 和 all_y 中
    # all_x 存储了二维平面上所有点的横坐标
    # all_y 存储了二维平面上所欲点的纵坐标
    # 使用 list 的 append 方法添加元素
    all_x.append(cur_x)
    all_y.append(cur_y)
「绘图」

# 这里的 x, y 值为了绘制二次函数
# 的那根曲线用的，和算法没有关系
# linspace 将会从区间 [-5, 7] 中
# 等距离分割出 100 个点并返回一个
# np.array 类型的对象给 x
x = np.linspace(-5, 7, 100)
# 计算出 x 中每一个横坐标对应的纵坐标
y = (x-1)**2 + 1
# plot 函数会把传入的 x, y
# 组成的每一个点依次连接成一个平滑的曲线
# 这样就是我们看到的二次函数的曲线了
plt.plot(x, y)
# axis 函数用来指定坐标系的横轴纵轴的范围
# 这样就表示了 
# 横轴为 [-7, 9]
# 纵轴为 [0, 50]
plt.axis([-7, 9, 0, 50])
# scatter 函数是用来绘制散点图的
# scatter 和 plot 函数不同
# scatter 并不会将每个点依次连接
# 而是直接将它们以点的形式绘制出来
plt.scatter(np.array(all_x), np.array(all_y), color='red')
plt.show()
「图片效果」


通过最终的效果图可以发现，「梯度下降」在一步一步地收敛到二次函数的极小值点，同时也是二次函数的最小值点。仔细看一下，可以发现，就像刚才提到的，随着算法的运行，红色的点和点之间的距离越来越小了，也就是每一次更新的幅度越来越小了。这可不关「学习率」 的事儿，因为在这里用到的学习率是一个固定的值，这只是因为随着接近极值点的过程中「导数的绝对值」越来越小。

同时，我们还可以把 all_x 和 all_y 中最后一个位置存储的值打印出来看一下，如果没错的话，all_x 的最后一个位置的值是接近于 1 的，同时对应的函数值 all_y 最后一个位置的值也应该是接近于 1 的。


3 二元函数的梯度下降
在多元函数中拥有多个自变量，在这里就用二元的函数来举例子好了，二元函数的图像是在三维坐标系中的。下边儿就是一个多元函数的图像例子。在这里纵坐标我使用 「y」 表示，底面的两个坐标分别使用 「x1」 和 「x2」 来表示。不用太纠结底面到底哪个轴是 x1 哪个轴是 x2，这里只是为了说明问题。


「图片来自 维基百科」

通过上面的二元函数凸显可以发现想要让 「y」 的值尽可能地小就要寻找极值点（同样地这个极值点也不一定是最小值点）。梯度是一个由各个自变量的偏导数所组成的一个「向量」。用数学表达式看上去就是下边儿这样的。


算法的过程和上边儿举例子用的二次函数是一样的，为了直观，我就直接使用数学式子的方式表达出来。



同样地，一开始随机初始一个位置，随后让当前位置的坐标值减去学习率乘以当前位置的偏导数。更新自变量 x1 就让 x1 减去学习率乘以 y 对 x1 的偏导数，更新自变量 x2 就让 x2 减去学习率乘以 y 对 x2 的偏导数。

「发挥想象力」

发挥想象力再思考一下，既然梯度是一个向量，那么它就代表了一个方向。 x1 的偏导数的反方向告诉我们在 x1 这个轴上朝向哪个方向变化可以使得函数值 y 减小，x2 的偏导数的反方向告诉我们在 x2 这个轴上朝向哪个方向变化可以使得函数值 y 减小。那么，由偏导数组成的向量就告诉了我们在底面朝向哪个方向走就可以使得函数 y 减小。

在下边儿的图中，我使用红色箭头表示我们当前所在的位置，随后我使用 「黑色」 箭头代表其中一个轴上的坐标朝向哪个方向变化可以使得函数值 「y」 减小，并使用 「黄色」 箭头代表另外一个轴上的坐标朝向哪个方向变化可以使得函数值 「y」 减小。根据平行色变形法则有了 「黑色」 向量和 「黄色」 向量，就可以知道这两个向量最终达到的效果就是 「蓝色」 向量所达到的效果。


最终，对于二元函数梯度下降的理解，就可以理解为求出的梯度的反方向在底面给了我们一个方向，只要朝着这个方向变化底面的坐标就可以使得函数值 「y」 变小。

再看一下刚才的式子，是不是就觉得很亲切了呢。



其实上边儿的式子变成向量的形式，还可以写成下边儿这个样子。


4 多元函数的梯度下降
对于更加一般的形式，比二元更多的自变量的表达式就是下边儿这个样子了。由于超过了二元就已经没有办法画出来了。因为二元函数需要在三维坐标系下，三元函数需要在四维坐标系下，依次类推。


梯度下降法」 的目标是搜索出来一个能够让函数的值尽可能小的位置，所以应该让 
 朝着黑色箭头的方向走，那么怎么完成这个操作呢？接下来的过程我使用伪代码来表示。

在代码中有一个 
 变量，它的专业名词叫 「学习率」。使用数学表达式来表示更新 
 的过程那就是：


表达式的意思就是让 
 减去 
 乘以函数的导数。其中的 
 是为了控制 
 更新的幅度，将 
 设置的小一点，那么每一次更新的幅度就会小一点。这是有用的，在后边儿我会解释 
 的设置会出现什么样的问题。

「初始化：」

# 变量 x 表示当前所在的位置
# 就随机初始在 6 这个位置好了
x = 6
eta = 0.05
「第 1 次更新 x」

# 变量 df 存储当前位置的导数值
df = 2*x-2
x = x - eta*df
# 更新后 x 由 6 变成 
# 6-0.05*10 = 5.5
「第 2 次更新 x」

# 因为 x 的位置发生了变化
# 要重新计算当前位置的导数
df = 2*x-2
x = x - eta*df
# 更新后 x 由 5.5 变成
# 5.5-0.05*9 = 5.05
好啦，接下来只要重复刚才的过程就可以了。那么，把刚才的两个步骤画出来。其中红色箭头指向的点是初始时候的点，「黄色」 箭头指向的点是第 1 次更新后的位置，「蓝色」 箭头指向的点是第 2 次更新后的位置。


只要不停地重复刚才的这个过程，那么最终就会收敛到一个 「局部」 的极值点。那么可以想一下这是为什么？因为随着每一次的更新，曲线都会越来越平缓，相应的导数值也会越来越小，当我们接近极值点的时候导数的值会无限地靠近 「0」。由于导数的绝对值越来越小，那么随后更新的幅度也会越来越小，最终就会停留在极值点的位置了。接下来我将用 Python 来实现这个过程，并让刚才的步骤迭代 1000 次。

「算法初始化」

import matplotlib.pyplot as plt
import numpy as np

# 初始算法开始之前的坐标 
# cur_x 和 cur_y 
cur_x = 6
cur_y = (cur_x-1)**2 + 1
# 设置学习率 eta 为 0.05
eta = 0.05
# 变量 iter 用于存储迭代次数
# 这次我们迭代 1000 次
# 所以给它赋值 1000
iter = 1000
# 变量 cur_df 用于存储
# 当前位置的导数
# 一开始我给它赋值为 None
# 每一轮循环的时候为它更新值
cur_df = None

# all_x 用于存储
# 算法进行时所有点的横坐标
all_x = []
# all_y 用于存储
# 算法进行时所有点的纵坐标
all_y = []

# 把最一开始的坐标存储到
# all_x 和 all_y 中
all_x.append(cur_x)
all_y.append(cur_y)
「迭代 1000 次」

# 循环结束也就意味着算法的结束
for i in range(iter):
    # 每一次迭代之前先计算
    # 当前位置的梯度 cur_df
    # cur 是英文单词 current 
    cur_df = 2*cur_x - 2
    # 更新 cur_x 到下一个位置
    cur_x = cur_x - eta*cur_df
    # 更新下一个 cur_x 对应的 cur_y
    cur_y = (cur_x-1)**2 + 1

    # 其实 cur_y 并没有起到实际的计算作用
    # 在这里计算 cur_y 只是为了将每一次的
    # 点的坐标存储到 all_x 和 all_y 中
    # all_x 存储了二维平面上所有点的横坐标
    # all_y 存储了二维平面上所欲点的纵坐标
    # 使用 list 的 append 方法添加元素
    all_x.append(cur_x)
    all_y.append(cur_y)
「绘图」

# 这里的 x, y 值为了绘制二次函数
# 的那根曲线用的，和算法没有关系
# linspace 将会从区间 [-5, 7] 中
# 等距离分割出 100 个点并返回一个
# np.array 类型的对象给 x
x = np.linspace(-5, 7, 100)
# 计算出 x 中每一个横坐标对应的纵坐标
y = (x-1)**2 + 1
# plot 函数会把传入的 x, y
# 组成的每一个点依次连接成一个平滑的曲线
# 这样就是我们看到的二次函数的曲线了
plt.plot(x, y)
# axis 函数用来指定坐标系的横轴纵轴的范围
# 这样就表示了 
# 横轴为 [-7, 9]
# 纵轴为 [0, 50]
plt.axis([-7, 9, 0, 50])
# scatter 函数是用来绘制散点图的
# scatter 和 plot 函数不同
# scatter 并不会将每个点依次连接
# 而是直接将它们以点的形式绘制出来
plt.scatter(np.array(all_x), np.array(all_y), color='red')
plt.show()
「图片效果」


通过最终的效果图可以发现，「梯度下降」在一步一步地收敛到二次函数的极小值点，同时也是二次函数的最小值点。仔细看一下，可以发现，就像刚才提到的，随着算法的运行，红色的点和点之间的距离越来越小了，也就是每一次更新的幅度越来越小了。这可不关「学习率」 的事儿，因为在这里用到的学习率是一个固定的值，这只是因为随着接近极值点的过程中「导数的绝对值」越来越小。

同时，我们还可以把 all_x 和 all_y 中最后一个位置存储的值打印出来看一下，如果没错的话，all_x 的最后一个位置的值是接近于 1 的，同时对应的函数值 all_y 最后一个位置的值也应该是接近于 1 的。


3 二元函数的梯度下降
在多元函数中拥有多个自变量，在这里就用二元的函数来举例子好了，二元函数的图像是在三维坐标系中的。下边儿就是一个多元函数的图像例子。在这里纵坐标我使用 「y」 表示，底面的两个坐标分别使用 「x1」 和 「x2」 来表示。不用太纠结底面到底哪个轴是 x1 哪个轴是 x2，这里只是为了说明问题。


「图片来自 维基百科」

通过上面的二元函数凸显可以发现想要让 「y」 的值尽可能地小就要寻找极值点（同样地这个极值点也不一定是最小值点）。梯度是一个由各个自变量的偏导数所组成的一个「向量」。用数学表达式看上去就是下边儿这样的。


算法的过程和上边儿举例子用的二次函数是一样的，为了直观，我就直接使用数学式子的方式表达出来。



同样地，一开始随机初始一个位置，随后让当前位置的坐标值减去学习率乘以当前位置的偏导数。更新自变量 x1 就让 x1 减去学习率乘以 y 对 x1 的偏导数，更新自变量 x2 就让 x2 减去学习率乘以 y 对 x2 的偏导数。

「发挥想象力」

发挥想象力再思考一下，既然梯度是一个向量，那么它就代表了一个方向。 x1 的偏导数的反方向告诉我们在 x1 这个轴上朝向哪个方向变化可以使得函数值 y 减小，x2 的偏导数的反方向告诉我们在 x2 这个轴上朝向哪个方向变化可以使得函数值 y 减小。那么，由偏导数组成的向量就告诉了我们在底面朝向哪个方向走就可以使得函数 y 减小。

在下边儿的图中，我使用红色箭头表示我们当前所在的位置，随后我使用 「黑色」 箭头代表其中一个轴上的坐标朝向哪个方向变化可以使得函数值 「y」 减小，并使用 「黄色」 箭头代表另外一个轴上的坐标朝向哪个方向变化可以使得函数值 「y」 减小。根据平行色变形法则有了 「黑色」 向量和 「黄色」 向量，就可以知道这两个向量最终达到的效果就是 「蓝色」 向量所达到的效果。


最终，对于二元函数梯度下降的理解，就可以理解为求出的梯度的反方向在底面给了我们一个方向，只要朝着这个方向变化底面的坐标就可以使得函数值 「y」 变小。

再看一下刚才的式子，是不是就觉得很亲切了呢。



其实上边儿的式子变成向量的形式，还可以写成下边儿这个样子。


4 多元函数的梯度下降
对于更加一般的形式，比二元更多的自变量的表达式就是下边儿这个样子了。由于超过了二元就已经没有办法画出来了。因为二元函数需要在三维坐标系下，三元函数需要在四维坐标系下，依次类推。


梯度下降法」 的目标是搜索出来一个能够让函数的值尽可能小的位置，所以应该让 
 朝着黑色箭头的方向走，那么怎么完成这个操作呢？接下来的过程我使用伪代码来表示。

在代码中有一个 
 变量，它的专业名词叫 「学习率」。使用数学表达式来表示更新 
 的过程那就是：


表达式的意思就是让 
 减去 
 乘以函数的导数。其中的 
 是为了控制 
 更新的幅度，将 
 设置的小一点，那么每一次更新的幅度就会小一点。这是有用的，在后边儿我会解释 
 的设置会出现什么样的问题。

「初始化：」

# 变量 x 表示当前所在的位置
# 就随机初始在 6 这个位置好了
x = 6
eta = 0.05
「第 1 次更新 x」

# 变量 df 存储当前位置的导数值
df = 2*x-2
x = x - eta*df
# 更新后 x 由 6 变成 
# 6-0.05*10 = 5.5
「第 2 次更新 x」

# 因为 x 的位置发生了变化
# 要重新计算当前位置的导数
df = 2*x-2
x = x - eta*df
# 更新后 x 由 5.5 变成
# 5.5-0.05*9 = 5.05
好啦，接下来只要重复刚才的过程就可以了。那么，把刚才的两个步骤画出来。其中红色箭头指向的点是初始时候的点，「黄色」 箭头指向的点是第 1 次更新后的位置，「蓝色」 箭头指向的点是第 2 次更新后的位置。


只要不停地重复刚才的这个过程，那么最终就会收敛到一个 「局部」 的极值点。那么可以想一下这是为什么？因为随着每一次的更新，曲线都会越来越平缓，相应的导数值也会越来越小，当我们接近极值点的时候导数的值会无限地靠近 「0」。由于导数的绝对值越来越小，那么随后更新的幅度也会越来越小，最终就会停留在极值点的位置了。接下来我将用 Python 来实现这个过程，并让刚才的步骤迭代 1000 次。

「算法初始化」

import matplotlib.pyplot as plt
import numpy as np

# 初始算法开始之前的坐标 
# cur_x 和 cur_y 
cur_x = 6
cur_y = (cur_x-1)**2 + 1
# 设置学习率 eta 为 0.05
eta = 0.05
# 变量 iter 用于存储迭代次数
# 这次我们迭代 1000 次
# 所以给它赋值 1000
iter = 1000
# 变量 cur_df 用于存储
# 当前位置的导数
# 一开始我给它赋值为 None
# 每一轮循环的时候为它更新值
cur_df = None

# all_x 用于存储
# 算法进行时所有点的横坐标
all_x = []
# all_y 用于存储
# 算法进行时所有点的纵坐标
all_y = []

# 把最一开始的坐标存储到
# all_x 和 all_y 中
all_x.append(cur_x)
all_y.append(cur_y)
「迭代 1000 次」

# 循环结束也就意味着算法的结束
for i in range(iter):
    # 每一次迭代之前先计算
    # 当前位置的梯度 cur_df
    # cur 是英文单词 current 
    cur_df = 2*cur_x - 2
    # 更新 cur_x 到下一个位置
    cur_x = cur_x - eta*cur_df
    # 更新下一个 cur_x 对应的 cur_y
    cur_y = (cur_x-1)**2 + 1

    # 其实 cur_y 并没有起到实际的计算作用
    # 在这里计算 cur_y 只是为了将每一次的
    # 点的坐标存储到 all_x 和 all_y 中
    # all_x 存储了二维平面上所有点的横坐标
    # all_y 存储了二维平面上所欲点的纵坐标
    # 使用 list 的 append 方法添加元素
    all_x.append(cur_x)
    all_y.append(cur_y)
「绘图」

# 这里的 x, y 值为了绘制二次函数
# 的那根曲线用的，和算法没有关系
# linspace 将会从区间 [-5, 7] 中
# 等距离分割出 100 个点并返回一个
# np.array 类型的对象给 x
x = np.linspace(-5, 7, 100)
# 计算出 x 中每一个横坐标对应的纵坐标
y = (x-1)**2 + 1
# plot 函数会把传入的 x, y
# 组成的每一个点依次连接成一个平滑的曲线
# 这样就是我们看到的二次函数的曲线了
plt.plot(x, y)
# axis 函数用来指定坐标系的横轴纵轴的范围
# 这样就表示了 
# 横轴为 [-7, 9]
# 纵轴为 [0, 50]
plt.axis([-7, 9, 0, 50])
# scatter 函数是用来绘制散点图的
# scatter 和 plot 函数不同
# scatter 并不会将每个点依次连接
# 而是直接将它们以点的形式绘制出来
plt.scatter(np.array(all_x), np.array(all_y), color='red')
plt.show()
「图片效果」


通过最终的效果图可以发现，「梯度下降」在一步一步地收敛到二次函数的极小值点，同时也是二次函数的最小值点。仔细看一下，可以发现，就像刚才提到的，随着算法的运行，红色的点和点之间的距离越来越小了，也就是每一次更新的幅度越来越小了。这可不关「学习率」 的事儿，因为在这里用到的学习率是一个固定的值，这只是因为随着接近极值点的过程中「导数的绝对值」越来越小。

同时，我们还可以把 all_x 和 all_y 中最后一个位置存储的值打印出来看一下，如果没错的话，all_x 的最后一个位置的值是接近于 1 的，同时对应的函数值 all_y 最后一个位置的值也应该是接近于 1 的。


3 二元函数的梯度下降
在多元函数中拥有多个自变量，在这里就用二元的函数来举例子好了，二元函数的图像是在三维坐标系中的。下边儿就是一个多元函数的图像例子。在这里纵坐标我使用 「y」 表示，底面的两个坐标分别使用 「x1」 和 「x2」 来表示。不用太纠结底面到底哪个轴是 x1 哪个轴是 x2，这里只是为了说明问题。


「图片来自 维基百科」

通过上面的二元函数凸显可以发现想要让 「y」 的值尽可能地小就要寻找极值点（同样地这个极值点也不一定是最小值点）。梯度是一个由各个自变量的偏导数所组成的一个「向量」。用数学表达式看上去就是下边儿这样的。


算法的过程和上边儿举例子用的二次函数是一样的，为了直观，我就直接使用数学式子的方式表达出来。



同样地，一开始随机初始一个位置，随后让当前位置的坐标值减去学习率乘以当前位置的偏导数。更新自变量 x1 就让 x1 减去学习率乘以 y 对 x1 的偏导数，更新自变量 x2 就让 x2 减去学习率乘以 y 对 x2 的偏导数。

「发挥想象力」

发挥想象力再思考一下，既然梯度是一个向量，那么它就代表了一个方向。 x1 的偏导数的反方向告诉我们在 x1 这个轴上朝向哪个方向变化可以使得函数值 y 减小，x2 的偏导数的反方向告诉我们在 x2 这个轴上朝向哪个方向变化可以使得函数值 y 减小。那么，由偏导数组成的向量就告诉了我们在底面朝向哪个方向走就可以使得函数 y 减小。

在下边儿的图中，我使用红色箭头表示我们当前所在的位置，随后我使用 「黑色」 箭头代表其中一个轴上的坐标朝向哪个方向变化可以使得函数值 「y」 减小，并使用 「黄色」 箭头代表另外一个轴上的坐标朝向哪个方向变化可以使得函数值 「y」 减小。根据平行色变形法则有了 「黑色」 向量和 「黄色」 向量，就可以知道这两个向量最终达到的效果就是 「蓝色」 向量所达到的效果。


最终，对于二元函数梯度下降的理解，就可以理解为求出的梯度的反方向在底面给了我们一个方向，只要朝着这个方向变化底面的坐标就可以使得函数值 「y」 变小。

再看一下刚才的式子，是不是就觉得很亲切了呢。



其实上边儿的式子变成向量的形式，还可以写成下边儿这个样子。


4 多元函数的梯度下降
对于更加一般的形式，比二元更多的自变量的表达式就是下边儿这个样子了。由于超过了二元就已经没有办法画出来了。因为二元函数需要在三维坐标系下，三元函数需要在四维坐标系下，依次类推。


梯度下降法」 的目标是搜索出来一个能够让函数的值尽可能小的位置，所以应该让 
 朝着黑色箭头的方向走，那么怎么完成这个操作呢？接下来的过程我使用伪代码来表示。

在代码中有一个 
 变量，它的专业名词叫 「学习率」。使用数学表达式来表示更新 
 的过程那就是：


表达式的意思就是让 
 减去 
 乘以函数的导数。其中的 
 是为了控制 
 更新的幅度，将 
 设置的小一点，那么每一次更新的幅度就会小一点。这是有用的，在后边儿我会解释 
 的设置会出现什么样的问题。

「初始化：」

# 变量 x 表示当前所在的位置
# 就随机初始在 6 这个位置好了
x = 6
eta = 0.05
「第 1 次更新 x」

# 变量 df 存储当前位置的导数值
df = 2*x-2
x = x - eta*df
# 更新后 x 由 6 变成 
# 6-0.05*10 = 5.5
「第 2 次更新 x」

# 因为 x 的位置发生了变化
# 要重新计算当前位置的导数
df = 2*x-2
x = x - eta*df
# 更新后 x 由 5.5 变成
# 5.5-0.05*9 = 5.05
好啦，接下来只要重复刚才的过程就可以了。那么，把刚才的两个步骤画出来。其中红色箭头指向的点是初始时候的点，「黄色」 箭头指向的点是第 1 次更新后的位置，「蓝色」 箭头指向的点是第 2 次更新后的位置。


只要不停地重复刚才的这个过程，那么最终就会收敛到一个 「局部」 的极值点。那么可以想一下这是为什么？因为随着每一次的更新，曲线都会越来越平缓，相应的导数值也会越来越小，当我们接近极值点的时候导数的值会无限地靠近 「0」。由于导数的绝对值越来越小，那么随后更新的幅度也会越来越小，最终就会停留在极值点的位置了。接下来我将用 Python 来实现这个过程，并让刚才的步骤迭代 1000 次。

「算法初始化」

import matplotlib.pyplot as plt
import numpy as np

# 初始算法开始之前的坐标 
# cur_x 和 cur_y 
cur_x = 6
cur_y = (cur_x-1)**2 + 1
# 设置学习率 eta 为 0.05
eta = 0.05
# 变量 iter 用于存储迭代次数
# 这次我们迭代 1000 次
# 所以给它赋值 1000
iter = 1000
# 变量 cur_df 用于存储
# 当前位置的导数
# 一开始我给它赋值为 None
# 每一轮循环的时候为它更新值
cur_df = None

# all_x 用于存储
# 算法进行时所有点的横坐标
all_x = []
# all_y 用于存储
# 算法进行时所有点的纵坐标
all_y = []

# 把最一开始的坐标存储到
# all_x 和 all_y 中
all_x.append(cur_x)
all_y.append(cur_y)
「迭代 1000 次」

# 循环结束也就意味着算法的结束
for i in range(iter):
    # 每一次迭代之前先计算
    # 当前位置的梯度 cur_df
    # cur 是英文单词 current 
    cur_df = 2*cur_x - 2
    # 更新 cur_x 到下一个位置
    cur_x = cur_x - eta*cur_df
    # 更新下一个 cur_x 对应的 cur_y
    cur_y = (cur_x-1)**2 + 1

    # 其实 cur_y 并没有起到实际的计算作用
    # 在这里计算 cur_y 只是为了将每一次的
    # 点的坐标存储到 all_x 和 all_y 中
    # all_x 存储了二维平面上所有点的横坐标
    # all_y 存储了二维平面上所欲点的纵坐标
    # 使用 list 的 append 方法添加元素
    all_x.append(cur_x)
    all_y.append(cur_y)
「绘图」

# 这里的 x, y 值为了绘制二次函数
# 的那根曲线用的，和算法没有关系
# linspace 将会从区间 [-5, 7] 中
# 等距离分割出 100 个点并返回一个
# np.array 类型的对象给 x
x = np.linspace(-5, 7, 100)
# 计算出 x 中每一个横坐标对应的纵坐标
y = (x-1)**2 + 1
# plot 函数会把传入的 x, y
# 组成的每一个点依次连接成一个平滑的曲线
# 这样就是我们看到的二次函数的曲线了
plt.plot(x, y)
# axis 函数用来指定坐标系的横轴纵轴的范围
# 这样就表示了 
# 横轴为 [-7, 9]
# 纵轴为 [0, 50]
plt.axis([-7, 9, 0, 50])
# scatter 函数是用来绘制散点图的
# scatter 和 plot 函数不同
# scatter 并不会将每个点依次连接
# 而是直接将它们以点的形式绘制出来
plt.scatter(np.array(all_x), np.array(all_y), color='red')
plt.show()
「图片效果」


通过最终的效果图可以发现，「梯度下降」在一步一步地收敛到二次函数的极小值点，同时也是二次函数的最小值点。仔细看一下，可以发现，就像刚才提到的，随着算法的运行，红色的点和点之间的距离越来越小了，也就是每一次更新的幅度越来越小了。这可不关「学习率」 的事儿，因为在这里用到的学习率是一个固定的值，这只是因为随着接近极值点的过程中「导数的绝对值」越来越小。

同时，我们还可以把 all_x 和 all_y 中最后一个位置存储的值打印出来看一下，如果没错的话，all_x 的最后一个位置的值是接近于 1 的，同时对应的函数值 all_y 最后一个位置的值也应该是接近于 1 的。


3 二元函数的梯度下降
在多元函数中拥有多个自变量，在这里就用二元的函数来举例子好了，二元函数的图像是在三维坐标系中的。下边儿就是一个多元函数的图像例子。在这里纵坐标我使用 「y」 表示，底面的两个坐标分别使用 「x1」 和 「x2」 来表示。不用太纠结底面到底哪个轴是 x1 哪个轴是 x2，这里只是为了说明问题。


「图片来自 维基百科」

通过上面的二元函数凸显可以发现想要让 「y」 的值尽可能地小就要寻找极值点（同样地这个极值点也不一定是最小值点）。梯度是一个由各个自变量的偏导数所组成的一个「向量」。用数学表达式看上去就是下边儿这样的。


算法的过程和上边儿举例子用的二次函数是一样的，为了直观，我就直接使用数学式子的方式表达出来。



同样地，一开始随机初始一个位置，随后让当前位置的坐标值减去学习率乘以当前位置的偏导数。更新自变量 x1 就让 x1 减去学习率乘以 y 对 x1 的偏导数，更新自变量 x2 就让 x2 减去学习率乘以 y 对 x2 的偏导数。

「发挥想象力」

发挥想象力再思考一下，既然梯度是一个向量，那么它就代表了一个方向。 x1 的偏导数的反方向告诉我们在 x1 这个轴上朝向哪个方向变化可以使得函数值 y 减小，x2 的偏导数的反方向告诉我们在 x2 这个轴上朝向哪个方向变化可以使得函数值 y 减小。那么，由偏导数组成的向量就告诉了我们在底面朝向哪个方向走就可以使得函数 y 减小。

在下边儿的图中，我使用红色箭头表示我们当前所在的位置，随后我使用 「黑色」 箭头代表其中一个轴上的坐标朝向哪个方向变化可以使得函数值 「y」 减小，并使用 「黄色」 箭头代表另外一个轴上的坐标朝向哪个方向变化可以使得函数值 「y」 减小。根据平行色变形法则有了 「黑色」 向量和 「黄色」 向量，就可以知道这两个向量最终达到的效果就是 「蓝色」 向量所达到的效果。


最终，对于二元函数梯度下降的理解，就可以理解为求出的梯度的反方向在底面给了我们一个方向，只要朝着这个方向变化底面的坐标就可以使得函数值 「y」 变小。

再看一下刚才的式子，是不是就觉得很亲切了呢。



其实上边儿的式子变成向量的形式，还可以写成下边儿这个样子。


4 多元函数的梯度下降
对于更加一般的形式，比二元更多的自变量的表达式就是下边儿这个样子了。由于超过了二元就已经没有办法画出来了。因为二元函数需要在三维坐标系下，三元函数需要在四维坐标系下，依次类推。


梯度下降法」 的目标是搜索出来一个能够让函数的值尽可能小的位置，所以应该让 
 朝着黑色箭头的方向走，那么怎么完成这个操作呢？接下来的过程我使用伪代码来表示。

在代码中有一个 
 变量，它的专业名词叫 「学习率」。使用数学表达式来表示更新 
 的过程那就是：


表达式的意思就是让 
 减去 
 乘以函数的导数。其中的 
 是为了控制 
 更新的幅度，将 
 设置的小一点，那么每一次更新的幅度就会小一点。这是有用的，在后边儿我会解释 
 的设置会出现什么样的问题。

「初始化：」

# 变量 x 表示当前所在的位置
# 就随机初始在 6 这个位置好了
x = 6
eta = 0.05
「第 1 次更新 x」

# 变量 df 存储当前位置的导数值
df = 2*x-2
x = x - eta*df
# 更新后 x 由 6 变成 
# 6-0.05*10 = 5.5
「第 2 次更新 x」

# 因为 x 的位置发生了变化
# 要重新计算当前位置的导数
df = 2*x-2
x = x - eta*df
# 更新后 x 由 5.5 变成
# 5.5-0.05*9 = 5.05
好啦，接下来只要重复刚才的过程就可以了。那么，把刚才的两个步骤画出来。其中红色箭头指向的点是初始时候的点，「黄色」 箭头指向的点是第 1 次更新后的位置，「蓝色」 箭头指向的点是第 2 次更新后的位置。


只要不停地重复刚才的这个过程，那么最终就会收敛到一个 「局部」 的极值点。那么可以想一下这是为什么？因为随着每一次的更新，曲线都会越来越平缓，相应的导数值也会越来越小，当我们接近极值点的时候导数的值会无限地靠近 「0」。由于导数的绝对值越来越小，那么随后更新的幅度也会越来越小，最终就会停留在极值点的位置了。接下来我将用 Python 来实现这个过程，并让刚才的步骤迭代 1000 次。

「算法初始化」

import matplotlib.pyplot as plt
import numpy as np

# 初始算法开始之前的坐标 
# cur_x 和 cur_y 
cur_x = 6
cur_y = (cur_x-1)**2 + 1
# 设置学习率 eta 为 0.05
eta = 0.05
# 变量 iter 用于存储迭代次数
# 这次我们迭代 1000 次
# 所以给它赋值 1000
iter = 1000
# 变量 cur_df 用于存储
# 当前位置的导数
# 一开始我给它赋值为 None
# 每一轮循环的时候为它更新值
cur_df = None

# all_x 用于存储
# 算法进行时所有点的横坐标
all_x = []
# all_y 用于存储
# 算法进行时所有点的纵坐标
all_y = []

# 把最一开始的坐标存储到
# all_x 和 all_y 中
all_x.append(cur_x)
all_y.append(cur_y)
「迭代 1000 次」

# 循环结束也就意味着算法的结束
for i in range(iter):
    # 每一次迭代之前先计算
    # 当前位置的梯度 cur_df
    # cur 是英文单词 current 
    cur_df = 2*cur_x - 2
    # 更新 cur_x 到下一个位置
    cur_x = cur_x - eta*cur_df
    # 更新下一个 cur_x 对应的 cur_y
    cur_y = (cur_x-1)**2 + 1

    # 其实 cur_y 并没有起到实际的计算作用
    # 在这里计算 cur_y 只是为了将每一次的
    # 点的坐标存储到 all_x 和 all_y 中
    # all_x 存储了二维平面上所有点的横坐标
    # all_y 存储了二维平面上所欲点的纵坐标
    # 使用 list 的 append 方法添加元素
    all_x.append(cur_x)
    all_y.append(cur_y)
「绘图」

# 这里的 x, y 值为了绘制二次函数
# 的那根曲线用的，和算法没有关系
# linspace 将会从区间 [-5, 7] 中
# 等距离分割出 100 个点并返回一个
# np.array 类型的对象给 x
x = np.linspace(-5, 7, 100)
# 计算出 x 中每一个横坐标对应的纵坐标
y = (x-1)**2 + 1
# plot 函数会把传入的 x, y
# 组成的每一个点依次连接成一个平滑的曲线
# 这样就是我们看到的二次函数的曲线了
plt.plot(x, y)
# axis 函数用来指定坐标系的横轴纵轴的范围
# 这样就表示了 
# 横轴为 [-7, 9]
# 纵轴为 [0, 50]
plt.axis([-7, 9, 0, 50])
# scatter 函数是用来绘制散点图的
# scatter 和 plot 函数不同
# scatter 并不会将每个点依次连接
# 而是直接将它们以点的形式绘制出来
plt.scatter(np.array(all_x), np.array(all_y), color='red')
plt.show()
「图片效果」


通过最终的效果图可以发现，「梯度下降」在一步一步地收敛到二次函数的极小值点，同时也是二次函数的最小值点。仔细看一下，可以发现，就像刚才提到的，随着算法的运行，红色的点和点之间的距离越来越小了，也就是每一次更新的幅度越来越小了。这可不关「学习率」 的事儿，因为在这里用到的学习率是一个固定的值，这只是因为随着接近极值点的过程中「导数的绝对值」越来越小。

同时，我们还可以把 all_x 和 all_y 中最后一个位置存储的值打印出来看一下，如果没错的话，all_x 的最后一个位置的值是接近于 1 的，同时对应的函数值 all_y 最后一个位置的值也应该是接近于 1 的。


3 二元函数的梯度下降
在多元函数中拥有多个自变量，在这里就用二元的函数来举例子好了，二元函数的图像是在三维坐标系中的。下边儿就是一个多元函数的图像例子。在这里纵坐标我使用 「y」 表示，底面的两个坐标分别使用 「x1」 和 「x2」 来表示。不用太纠结底面到底哪个轴是 x1 哪个轴是 x2，这里只是为了说明问题。


「图片来自 维基百科」

通过上面的二元函数凸显可以发现想要让 「y」 的值尽可能地小就要寻找极值点（同样地这个极值点也不一定是最小值点）。梯度是一个由各个自变量的偏导数所组成的一个「向量」。用数学表达式看上去就是下边儿这样的。


算法的过程和上边儿举例子用的二次函数是一样的，为了直观，我就直接使用数学式子的方式表达出来。



同样地，一开始随机初始一个位置，随后让当前位置的坐标值减去学习率乘以当前位置的偏导数。更新自变量 x1 就让 x1 减去学习率乘以 y 对 x1 的偏导数，更新自变量 x2 就让 x2 减去学习率乘以 y 对 x2 的偏导数。

「发挥想象力」

发挥想象力再思考一下，既然梯度是一个向量，那么它就代表了一个方向。 x1 的偏导数的反方向告诉我们在 x1 这个轴上朝向哪个方向变化可以使得函数值 y 减小，x2 的偏导数的反方向告诉我们在 x2 这个轴上朝向哪个方向变化可以使得函数值 y 减小。那么，由偏导数组成的向量就告诉了我们在底面朝向哪个方向走就可以使得函数 y 减小。

在下边儿的图中，我使用红色箭头表示我们当前所在的位置，随后我使用 「黑色」 箭头代表其中一个轴上的坐标朝向哪个方向变化可以使得函数值 「y」 减小，并使用 「黄色」 箭头代表另外一个轴上的坐标朝向哪个方向变化可以使得函数值 「y」 减小。根据平行色变形法则有了 「黑色」 向量和 「黄色」 向量，就可以知道这两个向量最终达到的效果就是 「蓝色」 向量所达到的效果。


最终，对于二元函数梯度下降的理解，就可以理解为求出的梯度的反方向在底面给了我们一个方向，只要朝着这个方向变化底面的坐标就可以使得函数值 「y」 变小。

再看一下刚才的式子，是不是就觉得很亲切了呢。



其实上边儿的式子变成向量的形式，还可以写成下边儿这个样子。


4 多元函数的梯度下降
对于更加一般的形式，比二元更多的自变量的表达式就是下边儿这个样子了。由于超过了二元就已经没有办法画出来了。因为二元函数需要在三维坐标系下，三元函数需要在四维坐标系下，依次类推。


梯度下降法」 的目标是搜索出来一个能够让函数的值尽可能小的位置，所以应该让 
 朝着黑色箭头的方向走，那么怎么完成这个操作呢？接下来的过程我使用伪代码来表示。

在代码中有一个 
 变量，它的专业名词叫 「学习率」。使用数学表达式来表示更新 
 的过程那就是：


表达式的意思就是让 
 减去 
 乘以函数的导数。其中的 
 是为了控制 
 更新的幅度，将 
 设置的小一点，那么每一次更新的幅度就会小一点。这是有用的，在后边儿我会解释 
 的设置会出现什么样的问题。

「初始化：」

# 变量 x 表示当前所在的位置
# 就随机初始在 6 这个位置好了
x = 6
eta = 0.05
「第 1 次更新 x」

# 变量 df 存储当前位置的导数值
df = 2*x-2
x = x - eta*df
# 更新后 x 由 6 变成 
# 6-0.05*10 = 5.5
「第 2 次更新 x」

# 因为 x 的位置发生了变化
# 要重新计算当前位置的导数
df = 2*x-2
x = x - eta*df
# 更新后 x 由 5.5 变成
# 5.5-0.05*9 = 5.05
好啦，接下来只要重复刚才的过程就可以了。那么，把刚才的两个步骤画出来。其中红色箭头指向的点是初始时候的点，「黄色」 箭头指向的点是第 1 次更新后的位置，「蓝色」 箭头指向的点是第 2 次更新后的位置。


只要不停地重复刚才的这个过程，那么最终就会收敛到一个 「局部」 的极值点。那么可以想一下这是为什么？因为随着每一次的更新，曲线都会越来越平缓，相应的导数值也会越来越小，当我们接近极值点的时候导数的值会无限地靠近 「0」。由于导数的绝对值越来越小，那么随后更新的幅度也会越来越小，最终就会停留在极值点的位置了。接下来我将用 Python 来实现这个过程，并让刚才的步骤迭代 1000 次。

「算法初始化」

import matplotlib.pyplot as plt
import numpy as np

# 初始算法开始之前的坐标 
# cur_x 和 cur_y 
cur_x = 6
cur_y = (cur_x-1)**2 + 1
# 设置学习率 eta 为 0.05
eta = 0.05
# 变量 iter 用于存储迭代次数
# 这次我们迭代 1000 次
# 所以给它赋值 1000
iter = 1000
# 变量 cur_df 用于存储
# 当前位置的导数
# 一开始我给它赋值为 None
# 每一轮循环的时候为它更新值
cur_df = None

# all_x 用于存储
# 算法进行时所有点的横坐标
all_x = []
# all_y 用于存储
# 算法进行时所有点的纵坐标
all_y = []

# 把最一开始的坐标存储到
# all_x 和 all_y 中
all_x.append(cur_x)
all_y.append(cur_y)
「迭代 1000 次」

# 循环结束也就意味着算法的结束
for i in range(iter):
    # 每一次迭代之前先计算
    # 当前位置的梯度 cur_df
    # cur 是英文单词 current 
    cur_df = 2*cur_x - 2
    # 更新 cur_x 到下一个位置
    cur_x = cur_x - eta*cur_df
    # 更新下一个 cur_x 对应的 cur_y
    cur_y = (cur_x-1)**2 + 1

    # 其实 cur_y 并没有起到实际的计算作用
    # 在这里计算 cur_y 只是为了将每一次的
    # 点的坐标存储到 all_x 和 all_y 中
    # all_x 存储了二维平面上所有点的横坐标
    # all_y 存储了二维平面上所欲点的纵坐标
    # 使用 list 的 append 方法添加元素
    all_x.append(cur_x)
    all_y.append(cur_y)
「绘图」

# 这里的 x, y 值为了绘制二次函数
# 的那根曲线用的，和算法没有关系
# linspace 将会从区间 [-5, 7] 中
# 等距离分割出 100 个点并返回一个
# np.array 类型的对象给 x
x = np.linspace(-5, 7, 100)
# 计算出 x 中每一个横坐标对应的纵坐标
y = (x-1)**2 + 1
# plot 函数会把传入的 x, y
# 组成的每一个点依次连接成一个平滑的曲线
# 这样就是我们看到的二次函数的曲线了
plt.plot(x, y)
# axis 函数用来指定坐标系的横轴纵轴的范围
# 这样就表示了 
# 横轴为 [-7, 9]
# 纵轴为 [0, 50]
plt.axis([-7, 9, 0, 50])
# scatter 函数是用来绘制散点图的
# scatter 和 plot 函数不同
# scatter 并不会将每个点依次连接
# 而是直接将它们以点的形式绘制出来
plt.scatter(np.array(all_x), np.array(all_y), color='red')
plt.show()
「图片效果」


通过最终的效果图可以发现，「梯度下降」在一步一步地收敛到二次函数的极小值点，同时也是二次函数的最小值点。仔细看一下，可以发现，就像刚才提到的，随着算法的运行，红色的点和点之间的距离越来越小了，也就是每一次更新的幅度越来越小了。这可不关「学习率」 的事儿，因为在这里用到的学习率是一个固定的值，这只是因为随着接近极值点的过程中「导数的绝对值」越来越小。

同时，我们还可以把 all_x 和 all_y 中最后一个位置存储的值打印出来看一下，如果没错的话，all_x 的最后一个位置的值是接近于 1 的，同时对应的函数值 all_y 最后一个位置的值也应该是接近于 1 的。


3 二元函数的梯度下降
在多元函数中拥有多个自变量，在这里就用二元的函数来举例子好了，二元函数的图像是在三维坐标系中的。下边儿就是一个多元函数的图像例子。在这里纵坐标我使用 「y」 表示，底面的两个坐标分别使用 「x1」 和 「x2」 来表示。不用太纠结底面到底哪个轴是 x1 哪个轴是 x2，这里只是为了说明问题。


「图片来自 维基百科」

通过上面的二元函数凸显可以发现想要让 「y」 的值尽可能地小就要寻找极值点（同样地这个极值点也不一定是最小值点）。梯度是一个由各个自变量的偏导数所组成的一个「向量」。用数学表达式看上去就是下边儿这样的。


算法的过程和上边儿举例子用的二次函数是一样的，为了直观，我就直接使用数学式子的方式表达出来。



同样地，一开始随机初始一个位置，随后让当前位置的坐标值减去学习率乘以当前位置的偏导数。更新自变量 x1 就让 x1 减去学习率乘以 y 对 x1 的偏导数，更新自变量 x2 就让 x2 减去学习率乘以 y 对 x2 的偏导数。

「发挥想象力」

发挥想象力再思考一下，既然梯度是一个向量，那么它就代表了一个方向。 x1 的偏导数的反方向告诉我们在 x1 这个轴上朝向哪个方向变化可以使得函数值 y 减小，x2 的偏导数的反方向告诉我们在 x2 这个轴上朝向哪个方向变化可以使得函数值 y 减小。那么，由偏导数组成的向量就告诉了我们在底面朝向哪个方向走就可以使得函数 y 减小。

在下边儿的图中，我使用红色箭头表示我们当前所在的位置，随后我使用 「黑色」 箭头代表其中一个轴上的坐标朝向哪个方向变化可以使得函数值 「y」 减小，并使用 「黄色」 箭头代表另外一个轴上的坐标朝向哪个方向变化可以使得函数值 「y」 减小。根据平行色变形法则有了 「黑色」 向量和 「黄色」 向量，就可以知道这两个向量最终达到的效果就是 「蓝色」 向量所达到的效果。


最终，对于二元函数梯度下降的理解，就可以理解为求出的梯度的反方向在底面给了我们一个方向，只要朝着这个方向变化底面的坐标就可以使得函数值 「y」 变小。

再看一下刚才的式子，是不是就觉得很亲切了呢。



其实上边儿的式子变成向量的形式，还可以写成下边儿这个样子。


4 多元函数的梯度下降
对于更加一般的形式，比二元更多的自变量的表达式就是下边儿这个样子了。由于超过了二元就已经没有办法画出来了。因为二元函数需要在三维坐标系下，三元函数需要在四维坐标系下，依次类推。


梯度下降法」 的目标是搜索出来一个能够让函数的值尽可能小的位置，所以应该让 
 朝着黑色箭头的方向走，那么怎么完成这个操作呢？接下来的过程我使用伪代码来表示。

在代码中有一个 
 变量，它的专业名词叫 「学习率」。使用数学表达式来表示更新 
 的过程那就是：


表达式的意思就是让 
 减去 
 乘以函数的导数。其中的 
 是为了控制 
 更新的幅度，将 
 设置的小一点，那么每一次更新的幅度就会小一点。这是有用的，在后边儿我会解释 
 的设置会出现什么样的问题。

「初始化：」

# 变量 x 表示当前所在的位置
# 就随机初始在 6 这个位置好了
x = 6
eta = 0.05
「第 1 次更新 x」

# 变量 df 存储当前位置的导数值
df = 2*x-2
x = x - eta*df
# 更新后 x 由 6 变成 
# 6-0.05*10 = 5.5
「第 2 次更新 x」

# 因为 x 的位置发生了变化
# 要重新计算当前位置的导数
df = 2*x-2
x = x - eta*df
# 更新后 x 由 5.5 变成
# 5.5-0.05*9 = 5.05
好啦，接下来只要重复刚才的过程就可以了。那么，把刚才的两个步骤画出来。其中红色箭头指向的点是初始时候的点，「黄色」 箭头指向的点是第 1 次更新后的位置，「蓝色」 箭头指向的点是第 2 次更新后的位置。


只要不停地重复刚才的这个过程，那么最终就会收敛到一个 「局部」 的极值点。那么可以想一下这是为什么？因为随着每一次的更新，曲线都会越来越平缓，相应的导数值也会越来越小，当我们接近极值点的时候导数的值会无限地靠近 「0」。由于导数的绝对值越来越小，那么随后更新的幅度也会越来越小，最终就会停留在极值点的位置了。接下来我将用 Python 来实现这个过程，并让刚才的步骤迭代 1000 次。

「算法初始化」

import matplotlib.pyplot as plt
import numpy as np

# 初始算法开始之前的坐标 
# cur_x 和 cur_y 
cur_x = 6
cur_y = (cur_x-1)**2 + 1
# 设置学习率 eta 为 0.05
eta = 0.05
# 变量 iter 用于存储迭代次数
# 这次我们迭代 1000 次
# 所以给它赋值 1000
iter = 1000
# 变量 cur_df 用于存储
# 当前位置的导数
# 一开始我给它赋值为 None
# 每一轮循环的时候为它更新值
cur_df = None

# all_x 用于存储
# 算法进行时所有点的横坐标
all_x = []
# all_y 用于存储
# 算法进行时所有点的纵坐标
all_y = []

# 把最一开始的坐标存储到
# all_x 和 all_y 中
all_x.append(cur_x)
all_y.append(cur_y)
「迭代 1000 次」

# 循环结束也就意味着算法的结束
for i in range(iter):
    # 每一次迭代之前先计算
    # 当前位置的梯度 cur_df
    # cur 是英文单词 current 
    cur_df = 2*cur_x - 2
    # 更新 cur_x 到下一个位置
    cur_x = cur_x - eta*cur_df
    # 更新下一个 cur_x 对应的 cur_y
    cur_y = (cur_x-1)**2 + 1

    # 其实 cur_y 并没有起到实际的计算作用
    # 在这里计算 cur_y 只是为了将每一次的
    # 点的坐标存储到 all_x 和 all_y 中
    # all_x 存储了二维平面上所有点的横坐标
    # all_y 存储了二维平面上所欲点的纵坐标
    # 使用 list 的 append 方法添加元素
    all_x.append(cur_x)
    all_y.append(cur_y)
「绘图」

# 这里的 x, y 值为了绘制二次函数
# 的那根曲线用的，和算法没有关系
# linspace 将会从区间 [-5, 7] 中
# 等距离分割出 100 个点并返回一个
# np.array 类型的对象给 x
x = np.linspace(-5, 7, 100)
# 计算出 x 中每一个横坐标对应的纵坐标
y = (x-1)**2 + 1
# plot 函数会把传入的 x, y
# 组成的每一个点依次连接成一个平滑的曲线
# 这样就是我们看到的二次函数的曲线了
plt.plot(x, y)
# axis 函数用来指定坐标系的横轴纵轴的范围
# 这样就表示了 
# 横轴为 [-7, 9]
# 纵轴为 [0, 50]
plt.axis([-7, 9, 0, 50])
# scatter 函数是用来绘制散点图的
# scatter 和 plot 函数不同
# scatter 并不会将每个点依次连接
# 而是直接将它们以点的形式绘制出来
plt.scatter(np.array(all_x), np.array(all_y), color='red')
plt.show()
「图片效果」


通过最终的效果图可以发现，「梯度下降」在一步一步地收敛到二次函数的极小值点，同时也是二次函数的最小值点。仔细看一下，可以发现，就像刚才提到的，随着算法的运行，红色的点和点之间的距离越来越小了，也就是每一次更新的幅度越来越小了。这可不关「学习率」 的事儿，因为在这里用到的学习率是一个固定的值，这只是因为随着接近极值点的过程中「导数的绝对值」越来越小。

同时，我们还可以把 all_x 和 all_y 中最后一个位置存储的值打印出来看一下，如果没错的话，all_x 的最后一个位置的值是接近于 1 的，同时对应的函数值 all_y 最后一个位置的值也应该是接近于 1 的。


3 二元函数的梯度下降
在多元函数中拥有多个自变量，在这里就用二元的函数来举例子好了，二元函数的图像是在三维坐标系中的。下边儿就是一个多元函数的图像例子。在这里纵坐标我使用 「y」 表示，底面的两个坐标分别使用 「x1」 和 「x2」 来表示。不用太纠结底面到底哪个轴是 x1 哪个轴是 x2，这里只是为了说明问题。


「图片来自 维基百科」

通过上面的二元函数凸显可以发现想要让 「y」 的值尽可能地小就要寻找极值点（同样地这个极值点也不一定是最小值点）。梯度是一个由各个自变量的偏导数所组成的一个「向量」。用数学表达式看上去就是下边儿这样的。


算法的过程和上边儿举例子用的二次函数是一样的，为了直观，我就直接使用数学式子的方式表达出来。



同样地，一开始随机初始一个位置，随后让当前位置的坐标值减去学习率乘以当前位置的偏导数。更新自变量 x1 就让 x1 减去学习率乘以 y 对 x1 的偏导数，更新自变量 x2 就让 x2 减去学习率乘以 y 对 x2 的偏导数。

「发挥想象力」

发挥想象力再思考一下，既然梯度是一个向量，那么它就代表了一个方向。 x1 的偏导数的反方向告诉我们在 x1 这个轴上朝向哪个方向变化可以使得函数值 y 减小，x2 的偏导数的反方向告诉我们在 x2 这个轴上朝向哪个方向变化可以使得函数值 y 减小。那么，由偏导数组成的向量就告诉了我们在底面朝向哪个方向走就可以使得函数 y 减小。

在下边儿的图中，我使用红色箭头表示我们当前所在的位置，随后我使用 「黑色」 箭头代表其中一个轴上的坐标朝向哪个方向变化可以使得函数值 「y」 减小，并使用 「黄色」 箭头代表另外一个轴上的坐标朝向哪个方向变化可以使得函数值 「y」 减小。根据平行色变形法则有了 「黑色」 向量和 「黄色」 向量，就可以知道这两个向量最终达到的效果就是 「蓝色」 向量所达到的效果。


最终，对于二元函数梯度下降的理解，就可以理解为求出的梯度的反方向在底面给了我们一个方向，只要朝着这个方向变化底面的坐标就可以使得函数值 「y」 变小。

再看一下刚才的式子，是不是就觉得很亲切了呢。



其实上边儿的式子变成向量的形式，还可以写成下边儿这个样子。


4 多元函数的梯度下降
对于更加一般的形式，比二元更多的自变量的表达式就是下边儿这个样子了。由于超过了二元就已经没有办法画出来了。因为二元函数需要在三维坐标系下，三元函数需要在四维坐标系下，依次类推。


梯度下降法」 的目标是搜索出来一个能够让函数的值尽可能小的位置，所以应该让 
 朝着黑色箭头的方向走，那么怎么完成这个操作呢？接下来的过程我使用伪代码来表示。

在代码中有一个 
 变量，它的专业名词叫 「学习率」。使用数学表达式来表示更新 
 的过程那就是：


表达式的意思就是让 
 减去 
 乘以函数的导数。其中的 
 是为了控制 
 更新的幅度，将 
 设置的小一点，那么每一次更新的幅度就会小一点。这是有用的，在后边儿我会解释 
 的设置会出现什么样的问题。

「初始化：」

# 变量 x 表示当前所在的位置
# 就随机初始在 6 这个位置好了
x = 6
eta = 0.05
「第 1 次更新 x」

# 变量 df 存储当前位置的导数值
df = 2*x-2
x = x - eta*df
# 更新后 x 由 6 变成 
# 6-0.05*10 = 5.5
「第 2 次更新 x」

# 因为 x 的位置发生了变化
# 要重新计算当前位置的导数
df = 2*x-2
x = x - eta*df
# 更新后 x 由 5.5 变成
# 5.5-0.05*9 = 5.05
好啦，接下来只要重复刚才的过程就可以了。那么，把刚才的两个步骤画出来。其中红色箭头指向的点是初始时候的点，「黄色」 箭头指向的点是第 1 次更新后的位置，「蓝色」 箭头指向的点是第 2 次更新后的位置。


只要不停地重复刚才的这个过程，那么最终就会收敛到一个 「局部」 的极值点。那么可以想一下这是为什么？因为随着每一次的更新，曲线都会越来越平缓，相应的导数值也会越来越小，当我们接近极值点的时候导数的值会无限地靠近 「0」。由于导数的绝对值越来越小，那么随后更新的幅度也会越来越小，最终就会停留在极值点的位置了。接下来我将用 Python 来实现这个过程，并让刚才的步骤迭代 1000 次。

「算法初始化」

import matplotlib.pyplot as plt
import numpy as np

# 初始算法开始之前的坐标 
# cur_x 和 cur_y 
cur_x = 6
cur_y = (cur_x-1)**2 + 1
# 设置学习率 eta 为 0.05
eta = 0.05
# 变量 iter 用于存储迭代次数
# 这次我们迭代 1000 次
# 所以给它赋值 1000
iter = 1000
# 变量 cur_df 用于存储
# 当前位置的导数
# 一开始我给它赋值为 None
# 每一轮循环的时候为它更新值
cur_df = None

# all_x 用于存储
# 算法进行时所有点的横坐标
all_x = []
# all_y 用于存储
# 算法进行时所有点的纵坐标
all_y = []

# 把最一开始的坐标存储到
# all_x 和 all_y 中
all_x.append(cur_x)
all_y.append(cur_y)
「迭代 1000 次」

# 循环结束也就意味着算法的结束
for i in range(iter):
    # 每一次迭代之前先计算
    # 当前位置的梯度 cur_df
    # cur 是英文单词 current 
    cur_df = 2*cur_x - 2
    # 更新 cur_x 到下一个位置
    cur_x = cur_x - eta*cur_df
    # 更新下一个 cur_x 对应的 cur_y
    cur_y = (cur_x-1)**2 + 1

    # 其实 cur_y 并没有起到实际的计算作用
    # 在这里计算 cur_y 只是为了将每一次的
    # 点的坐标存储到 all_x 和 all_y 中
    # all_x 存储了二维平面上所有点的横坐标
    # all_y 存储了二维平面上所欲点的纵坐标
    # 使用 list 的 append 方法添加元素
    all_x.append(cur_x)
    all_y.append(cur_y)
「绘图」

# 这里的 x, y 值为了绘制二次函数
# 的那根曲线用的，和算法没有关系
# linspace 将会从区间 [-5, 7] 中
# 等距离分割出 100 个点并返回一个
# np.array 类型的对象给 x
x = np.linspace(-5, 7, 100)
# 计算出 x 中每一个横坐标对应的纵坐标
y = (x-1)**2 + 1
# plot 函数会把传入的 x, y
# 组成的每一个点依次连接成一个平滑的曲线
# 这样就是我们看到的二次函数的曲线了
plt.plot(x, y)
# axis 函数用来指定坐标系的横轴纵轴的范围
# 这样就表示了 
# 横轴为 [-7, 9]
# 纵轴为 [0, 50]
plt.axis([-7, 9, 0, 50])
# scatter 函数是用来绘制散点图的
# scatter 和 plot 函数不同
# scatter 并不会将每个点依次连接
# 而是直接将它们以点的形式绘制出来
plt.scatter(np.array(all_x), np.array(all_y), color='red')
plt.show()
「图片效果」


通过最终的效果图可以发现，「梯度下降」在一步一步地收敛到二次函数的极小值点，同时也是二次函数的最小值点。仔细看一下，可以发现，就像刚才提到的，随着算法的运行，红色的点和点之间的距离越来越小了，也就是每一次更新的幅度越来越小了。这可不关「学习率」 的事儿，因为在这里用到的学习率是一个固定的值，这只是因为随着接近极值点的过程中「导数的绝对值」越来越小。

同时，我们还可以把 all_x 和 all_y 中最后一个位置存储的值打印出来看一下，如果没错的话，all_x 的最后一个位置的值是接近于 1 的，同时对应的函数值 all_y 最后一个位置的值也应该是接近于 1 的。


3 二元函数的梯度下降
在多元函数中拥有多个自变量，在这里就用二元的函数来举例子好了，二元函数的图像是在三维坐标系中的。下边儿就是一个多元函数的图像例子。在这里纵坐标我使用 「y」 表示，底面的两个坐标分别使用 「x1」 和 「x2」 来表示。不用太纠结底面到底哪个轴是 x1 哪个轴是 x2，这里只是为了说明问题。


「图片来自 维基百科」

通过上面的二元函数凸显可以发现想要让 「y」 的值尽可能地小就要寻找极值点（同样地这个极值点也不一定是最小值点）。梯度是一个由各个自变量的偏导数所组成的一个「向量」。用数学表达式看上去就是下边儿这样的。


算法的过程和上边儿举例子用的二次函数是一样的，为了直观，我就直接使用数学式子的方式表达出来。



同样地，一开始随机初始一个位置，随后让当前位置的坐标值减去学习率乘以当前位置的偏导数。更新自变量 x1 就让 x1 减去学习率乘以 y 对 x1 的偏导数，更新自变量 x2 就让 x2 减去学习率乘以 y 对 x2 的偏导数。

「发挥想象力」

发挥想象力再思考一下，既然梯度是一个向量，那么它就代表了一个方向。 x1 的偏导数的反方向告诉我们在 x1 这个轴上朝向哪个方向变化可以使得函数值 y 减小，x2 的偏导数的反方向告诉我们在 x2 这个轴上朝向哪个方向变化可以使得函数值 y 减小。那么，由偏导数组成的向量就告诉了我们在底面朝向哪个方向走就可以使得函数 y 减小。

在下边儿的图中，我使用红色箭头表示我们当前所在的位置，随后我使用 「黑色」 箭头代表其中一个轴上的坐标朝向哪个方向变化可以使得函数值 「y」 减小，并使用 「黄色」 箭头代表另外一个轴上的坐标朝向哪个方向变化可以使得函数值 「y」 减小。根据平行色变形法则有了 「黑色」 向量和 「黄色」 向量，就可以知道这两个向量最终达到的效果就是 「蓝色」 向量所达到的效果。


最终，对于二元函数梯度下降的理解，就可以理解为求出的梯度的反方向在底面给了我们一个方向，只要朝着这个方向变化底面的坐标就可以使得函数值 「y」 变小。

再看一下刚才的式子，是不是就觉得很亲切了呢。



其实上边儿的式子变成向量的形式，还可以写成下边儿这个样子。


4 多元函数的梯度下降
对于更加一般的形式，比二元更多的自变量的表达式就是下边儿这个样子了。由于超过了二元就已经没有办法画出来了。因为二元函数需要在三维坐标系下，三元函数需要在四维坐标系下，依次类推。


梯度下降法」 的目标是搜索出来一个能够让函数的值尽可能小的位置，所以应该让 
 朝着黑色箭头的方向走，那么怎么完成这个操作呢？接下来的过程我使用伪代码来表示。

在代码中有一个 
 变量，它的专业名词叫 「学习率」。使用数学表达式来表示更新 
 的过程那就是：


表达式的意思就是让 
 减去 
 乘以函数的导数。其中的 
 是为了控制 
 更新的幅度，将 
 设置的小一点，那么每一次更新的幅度就会小一点。这是有用的，在后边儿我会解释 
 的设置会出现什么样的问题。

「初始化：」

# 变量 x 表示当前所在的位置
# 就随机初始在 6 这个位置好了
x = 6
eta = 0.05
「第 1 次更新 x」

# 变量 df 存储当前位置的导数值
df = 2*x-2
x = x - eta*df
# 更新后 x 由 6 变成 
# 6-0.05*10 = 5.5
「第 2 次更新 x」

# 因为 x 的位置发生了变化
# 要重新计算当前位置的导数
df = 2*x-2
x = x - eta*df
# 更新后 x 由 5.5 变成
# 5.5-0.05*9 = 5.05
好啦，接下来只要重复刚才的过程就可以了。那么，把刚才的两个步骤画出来。其中红色箭头指向的点是初始时候的点，「黄色」 箭头指向的点是第 1 次更新后的位置，「蓝色」 箭头指向的点是第 2 次更新后的位置。


只要不停地重复刚才的这个过程，那么最终就会收敛到一个 「局部」 的极值点。那么可以想一下这是为什么？因为随着每一次的更新，曲线都会越来越平缓，相应的导数值也会越来越小，当我们接近极值点的时候导数的值会无限地靠近 「0」。由于导数的绝对值越来越小，那么随后更新的幅度也会越来越小，最终就会停留在极值点的位置了。接下来我将用 Python 来实现这个过程，并让刚才的步骤迭代 1000 次。

「算法初始化」

import matplotlib.pyplot as plt
import numpy as np

# 初始算法开始之前的坐标 
# cur_x 和 cur_y 
cur_x = 6
cur_y = (cur_x-1)**2 + 1
# 设置学习率 eta 为 0.05
eta = 0.05
# 变量 iter 用于存储迭代次数
# 这次我们迭代 1000 次
# 所以给它赋值 1000
iter = 1000
# 变量 cur_df 用于存储
# 当前位置的导数
# 一开始我给它赋值为 None
# 每一轮循环的时候为它更新值
cur_df = None

# all_x 用于存储
# 算法进行时所有点的横坐标
all_x = []
# all_y 用于存储
# 算法进行时所有点的纵坐标
all_y = []

# 把最一开始的坐标存储到
# all_x 和 all_y 中
all_x.append(cur_x)
all_y.append(cur_y)
「迭代 1000 次」

# 循环结束也就意味着算法的结束
for i in range(iter):
    # 每一次迭代之前先计算
    # 当前位置的梯度 cur_df
    # cur 是英文单词 current 
    cur_df = 2*cur_x - 2
    # 更新 cur_x 到下一个位置
    cur_x = cur_x - eta*cur_df
    # 更新下一个 cur_x 对应的 cur_y
    cur_y = (cur_x-1)**2 + 1

    # 其实 cur_y 并没有起到实际的计算作用
    # 在这里计算 cur_y 只是为了将每一次的
    # 点的坐标存储到 all_x 和 all_y 中
    # all_x 存储了二维平面上所有点的横坐标
    # all_y 存储了二维平面上所欲点的纵坐标
    # 使用 list 的 append 方法添加元素
    all_x.append(cur_x)
    all_y.append(cur_y)
「绘图」

# 这里的 x, y 值为了绘制二次函数
# 的那根曲线用的，和算法没有关系
# linspace 将会从区间 [-5, 7] 中
# 等距离分割出 100 个点并返回一个
# np.array 类型的对象给 x
x = np.linspace(-5, 7, 100)
# 计算出 x 中每一个横坐标对应的纵坐标
y = (x-1)**2 + 1
# plot 函数会把传入的 x, y
# 组成的每一个点依次连接成一个平滑的曲线
# 这样就是我们看到的二次函数的曲线了
plt.plot(x, y)
# axis 函数用来指定坐标系的横轴纵轴的范围
# 这样就表示了 
# 横轴为 [-7, 9]
# 纵轴为 [0, 50]
plt.axis([-7, 9, 0, 50])
# scatter 函数是用来绘制散点图的
# scatter 和 plot 函数不同
# scatter 并不会将每个点依次连接
# 而是直接将它们以点的形式绘制出来
plt.scatter(np.array(all_x), np.array(all_y), color='red')
plt.show()
「图片效果」


通过最终的效果图可以发现，「梯度下降」在一步一步地收敛到二次函数的极小值点，同时也是二次函数的最小值点。仔细看一下，可以发现，就像刚才提到的，随着算法的运行，红色的点和点之间的距离越来越小了，也就是每一次更新的幅度越来越小了。这可不关「学习率」 的事儿，因为在这里用到的学习率是一个固定的值，这只是因为随着接近极值点的过程中「导数的绝对值」越来越小。

同时，我们还可以把 all_x 和 all_y 中最后一个位置存储的值打印出来看一下，如果没错的话，all_x 的最后一个位置的值是接近于 1 的，同时对应的函数值 all_y 最后一个位置的值也应该是接近于 1 的。


3 二元函数的梯度下降
在多元函数中拥有多个自变量，在这里就用二元的函数来举例子好了，二元函数的图像是在三维坐标系中的。下边儿就是一个多元函数的图像例子。在这里纵坐标我使用 「y」 表示，底面的两个坐标分别使用 「x1」 和 「x2」 来表示。不用太纠结底面到底哪个轴是 x1 哪个轴是 x2，这里只是为了说明问题。


「图片来自 维基百科」

通过上面的二元函数凸显可以发现想要让 「y」 的值尽可能地小就要寻找极值点（同样地这个极值点也不一定是最小值点）。梯度是一个由各个自变量的偏导数所组成的一个「向量」。用数学表达式看上去就是下边儿这样的。


算法的过程和上边儿举例子用的二次函数是一样的，为了直观，我就直接使用数学式子的方式表达出来。



同样地，一开始随机初始一个位置，随后让当前位置的坐标值减去学习率乘以当前位置的偏导数。更新自变量 x1 就让 x1 减去学习率乘以 y 对 x1 的偏导数，更新自变量 x2 就让 x2 减去学习率乘以 y 对 x2 的偏导数。

「发挥想象力」

发挥想象力再思考一下，既然梯度是一个向量，那么它就代表了一个方向。 x1 的偏导数的反方向告诉我们在 x1 这个轴上朝向哪个方向变化可以使得函数值 y 减小，x2 的偏导数的反方向告诉我们在 x2 这个轴上朝向哪个方向变化可以使得函数值 y 减小。那么，由偏导数组成的向量就告诉了我们在底面朝向哪个方向走就可以使得函数 y 减小。

在下边儿的图中，我使用红色箭头表示我们当前所在的位置，随后我使用 「黑色」 箭头代表其中一个轴上的坐标朝向哪个方向变化可以使得函数值 「y」 减小，并使用 「黄色」 箭头代表另外一个轴上的坐标朝向哪个方向变化可以使得函数值 「y」 减小。根据平行色变形法则有了 「黑色」 向量和 「黄色」 向量，就可以知道这两个向量最终达到的效果就是 「蓝色」 向量所达到的效果。


最终，对于二元函数梯度下降的理解，就可以理解为求出的梯度的反方向在底面给了我们一个方向，只要朝着这个方向变化底面的坐标就可以使得函数值 「y」 变小。

再看一下刚才的式子，是不是就觉得很亲切了呢。



其实上边儿的式子变成向量的形式，还可以写成下边儿这个样子。


4 多元函数的梯度下降
对于更加一般的形式，比二元更多的自变量的表达式就是下边儿这个样子了。由于超过了二元就已经没有办法画出来了。因为二元函数需要在三维坐标系下，三元函数需要在四维坐标系下，依次类推。


梯度下降法」 的目标是搜索出来一个能够让函数的值尽可能小的位置，所以应该让 
 朝着黑色箭头的方向走，那么怎么完成这个操作呢？接下来的过程我使用伪代码来表示。

在代码中有一个 
 变量，它的专业名词叫 「学习率」。使用数学表达式来表示更新 
 的过程那就是：


表达式的意思就是让 
 减去 
 乘以函数的导数。其中的 
 是为了控制 
 更新的幅度，将 
 设置的小一点，那么每一次更新的幅度就会小一点。这是有用的，在后边儿我会解释 
 的设置会出现什么样的问题。

「初始化：」

# 变量 x 表示当前所在的位置
# 就随机初始在 6 这个位置好了
x = 6
eta = 0.05
「第 1 次更新 x」

# 变量 df 存储当前位置的导数值
df = 2*x-2
x = x - eta*df
# 更新后 x 由 6 变成 
# 6-0.05*10 = 5.5
「第 2 次更新 x」

# 因为 x 的位置发生了变化
# 要重新计算当前位置的导数
df = 2*x-2
x = x - eta*df
# 更新后 x 由 5.5 变成
# 5.5-0.05*9 = 5.05
好啦，接下来只要重复刚才的过程就可以了。那么，把刚才的两个步骤画出来。其中红色箭头指向的点是初始时候的点，「黄色」 箭头指向的点是第 1 次更新后的位置，「蓝色」 箭头指向的点是第 2 次更新后的位置。


只要不停地重复刚才的这个过程，那么最终就会收敛到一个 「局部」 的极值点。那么可以想一下这是为什么？因为随着每一次的更新，曲线都会越来越平缓，相应的导数值也会越来越小，当我们接近极值点的时候导数的值会无限地靠近 「0」。由于导数的绝对值越来越小，那么随后更新的幅度也会越来越小，最终就会停留在极值点的位置了。接下来我将用 Python 来实现这个过程，并让刚才的步骤迭代 1000 次。

「算法初始化」

import matplotlib.pyplot as plt
import numpy as np

# 初始算法开始之前的坐标 
# cur_x 和 cur_y 
cur_x = 6
cur_y = (cur_x-1)**2 + 1
# 设置学习率 eta 为 0.05
eta = 0.05
# 变量 iter 用于存储迭代次数
# 这次我们迭代 1000 次
# 所以给它赋值 1000
iter = 1000
# 变量 cur_df 用于存储
# 当前位置的导数
# 一开始我给它赋值为 None
# 每一轮循环的时候为它更新值
cur_df = None

# all_x 用于存储
# 算法进行时所有点的横坐标
all_x = []
# all_y 用于存储
# 算法进行时所有点的纵坐标
all_y = []

# 把最一开始的坐标存储到
# all_x 和 all_y 中
all_x.append(cur_x)
all_y.append(cur_y)
「迭代 1000 次」

# 循环结束也就意味着算法的结束
for i in range(iter):
    # 每一次迭代之前先计算
    # 当前位置的梯度 cur_df
    # cur 是英文单词 current 
    cur_df = 2*cur_x - 2
    # 更新 cur_x 到下一个位置
    cur_x = cur_x - eta*cur_df
    # 更新下一个 cur_x 对应的 cur_y
    cur_y = (cur_x-1)**2 + 1

    # 其实 cur_y 并没有起到实际的计算作用
    # 在这里计算 cur_y 只是为了将每一次的
    # 点的坐标存储到 all_x 和 all_y 中
    # all_x 存储了二维平面上所有点的横坐标
    # all_y 存储了二维平面上所欲点的纵坐标
    # 使用 list 的 append 方法添加元素
    all_x.append(cur_x)
    all_y.append(cur_y)
「绘图」

# 这里的 x, y 值为了绘制二次函数
# 的那根曲线用的，和算法没有关系
# linspace 将会从区间 [-5, 7] 中
# 等距离分割出 100 个点并返回一个
# np.array 类型的对象给 x
x = np.linspace(-5, 7, 100)
# 计算出 x 中每一个横坐标对应的纵坐标
y = (x-1)**2 + 1
# plot 函数会把传入的 x, y
# 组成的每一个点依次连接成一个平滑的曲线
# 这样就是我们看到的二次函数的曲线了
plt.plot(x, y)
# axis 函数用来指定坐标系的横轴纵轴的范围
# 这样就表示了 
# 横轴为 [-7, 9]
# 纵轴为 [0, 50]
plt.axis([-7, 9, 0, 50])
# scatter 函数是用来绘制散点图的
# scatter 和 plot 函数不同
# scatter 并不会将每个点依次连接
# 而是直接将它们以点的形式绘制出来
plt.scatter(np.array(all_x), np.array(all_y), color='red')
plt.show()
「图片效果」


通过最终的效果图可以发现，「梯度下降」在一步一步地收敛到二次函数的极小值点，同时也是二次函数的最小值点。仔细看一下，可以发现，就像刚才提到的，随着算法的运行，红色的点和点之间的距离越来越小了，也就是每一次更新的幅度越来越小了。这可不关「学习率」 的事儿，因为在这里用到的学习率是一个固定的值，这只是因为随着接近极值点的过程中「导数的绝对值」越来越小。

同时，我们还可以把 all_x 和 all_y 中最后一个位置存储的值打印出来看一下，如果没错的话，all_x 的最后一个位置的值是接近于 1 的，同时对应的函数值 all_y 最后一个位置的值也应该是接近于 1 的。


3 二元函数的梯度下降
在多元函数中拥有多个自变量，在这里就用二元的函数来举例子好了，二元函数的图像是在三维坐标系中的。下边儿就是一个多元函数的图像例子。在这里纵坐标我使用 「y」 表示，底面的两个坐标分别使用 「x1」 和 「x2」 来表示。不用太纠结底面到底哪个轴是 x1 哪个轴是 x2，这里只是为了说明问题。


「图片来自 维基百科」

通过上面的二元函数凸显可以发现想要让 「y」 的值尽可能地小就要寻找极值点（同样地这个极值点也不一定是最小值点）。梯度是一个由各个自变量的偏导数所组成的一个「向量」。用数学表达式看上去就是下边儿这样的。


算法的过程和上边儿举例子用的二次函数是一样的，为了直观，我就直接使用数学式子的方式表达出来。



同样地，一开始随机初始一个位置，随后让当前位置的坐标值减去学习率乘以当前位置的偏导数。更新自变量 x1 就让 x1 减去学习率乘以 y 对 x1 的偏导数，更新自变量 x2 就让 x2 减去学习率乘以 y 对 x2 的偏导数。

「发挥想象力」

发挥想象力再思考一下，既然梯度是一个向量，那么它就代表了一个方向。 x1 的偏导数的反方向告诉我们在 x1 这个轴上朝向哪个方向变化可以使得函数值 y 减小，x2 的偏导数的反方向告诉我们在 x2 这个轴上朝向哪个方向变化可以使得函数值 y 减小。那么，由偏导数组成的向量就告诉了我们在底面朝向哪个方向走就可以使得函数 y 减小。

在下边儿的图中，我使用红色箭头表示我们当前所在的位置，随后我使用 「黑色」 箭头代表其中一个轴上的坐标朝向哪个方向变化可以使得函数值 「y」 减小，并使用 「黄色」 箭头代表另外一个轴上的坐标朝向哪个方向变化可以使得函数值 「y」 减小。根据平行色变形法则有了 「黑色」 向量和 「黄色」 向量，就可以知道这两个向量最终达到的效果就是 「蓝色」 向量所达到的效果。


最终，对于二元函数梯度下降的理解，就可以理解为求出的梯度的反方向在底面给了我们一个方向，只要朝着这个方向变化底面的坐标就可以使得函数值 「y」 变小。

再看一下刚才的式子，是不是就觉得很亲切了呢。



其实上边儿的式子变成向量的形式，还可以写成下边儿这个样子。


4 多元函数的梯度下降
对于更加一般的形式，比二元更多的自变量的表达式就是下边儿这个样子了。由于超过了二元就已经没有办法画出来了。因为二元函数需要在三维坐标系下，三元函数需要在四维坐标系下，依次类推。


梯度下降法」 的目标是搜索出来一个能够让函数的值尽可能小的位置，所以应该让 
 朝着黑色箭头的方向走，那么怎么完成这个操作呢？接下来的过程我使用伪代码来表示。

在代码中有一个 
 变量，它的专业名词叫 「学习率」。使用数学表达式来表示更新 
 的过程那就是：


表达式的意思就是让 
 减去 
 乘以函数的导数。其中的 
 是为了控制 
 更新的幅度，将 
 设置的小一点，那么每一次更新的幅度就会小一点。这是有用的，在后边儿我会解释 
 的设置会出现什么样的问题。

「初始化：」

# 变量 x 表示当前所在的位置
# 就随机初始在 6 这个位置好了
x = 6
eta = 0.05
「第 1 次更新 x」

# 变量 df 存储当前位置的导数值
df = 2*x-2
x = x - eta*df
# 更新后 x 由 6 变成 
# 6-0.05*10 = 5.5
「第 2 次更新 x」

# 因为 x 的位置发生了变化
# 要重新计算当前位置的导数
df = 2*x-2
x = x - eta*df
# 更新后 x 由 5.5 变成
# 5.5-0.05*9 = 5.05
好啦，接下来只要重复刚才的过程就可以了。那么，把刚才的两个步骤画出来。其中红色箭头指向的点是初始时候的点，「黄色」 箭头指向的点是第 1 次更新后的位置，「蓝色」 箭头指向的点是第 2 次更新后的位置。


只要不停地重复刚才的这个过程，那么最终就会收敛到一个 「局部」 的极值点。那么可以想一下这是为什么？因为随着每一次的更新，曲线都会越来越平缓，相应的导数值也会越来越小，当我们接近极值点的时候导数的值会无限地靠近 「0」。由于导数的绝对值越来越小，那么随后更新的幅度也会越来越小，最终就会停留在极值点的位置了。接下来我将用 Python 来实现这个过程，并让刚才的步骤迭代 1000 次。

「算法初始化」

import matplotlib.pyplot as plt
import numpy as np

# 初始算法开始之前的坐标 
# cur_x 和 cur_y 
cur_x = 6
cur_y = (cur_x-1)**2 + 1
# 设置学习率 eta 为 0.05
eta = 0.05
# 变量 iter 用于存储迭代次数
# 这次我们迭代 1000 次
# 所以给它赋值 1000
iter = 1000
# 变量 cur_df 用于存储
# 当前位置的导数
# 一开始我给它赋值为 None
# 每一轮循环的时候为它更新值
cur_df = None

# all_x 用于存储
# 算法进行时所有点的横坐标
all_x = []
# all_y 用于存储
# 算法进行时所有点的纵坐标
all_y = []

# 把最一开始的坐标存储到
# all_x 和 all_y 中
all_x.append(cur_x)
all_y.append(cur_y)
「迭代 1000 次」

# 循环结束也就意味着算法的结束
for i in range(iter):
    # 每一次迭代之前先计算
    # 当前位置的梯度 cur_df
    # cur 是英文单词 current 
    cur_df = 2*cur_x - 2
    # 更新 cur_x 到下一个位置
    cur_x = cur_x - eta*cur_df
    # 更新下一个 cur_x 对应的 cur_y
    cur_y = (cur_x-1)**2 + 1

    # 其实 cur_y 并没有起到实际的计算作用
    # 在这里计算 cur_y 只是为了将每一次的
    # 点的坐标存储到 all_x 和 all_y 中
    # all_x 存储了二维平面上所有点的横坐标
    # all_y 存储了二维平面上所欲点的纵坐标
    # 使用 list 的 append 方法添加元素
    all_x.append(cur_x)
    all_y.append(cur_y)
「绘图」

# 这里的 x, y 值为了绘制二次函数
# 的那根曲线用的，和算法没有关系
# linspace 将会从区间 [-5, 7] 中
# 等距离分割出 100 个点并返回一个
# np.array 类型的对象给 x
x = np.linspace(-5, 7, 100)
# 计算出 x 中每一个横坐标对应的纵坐标
y = (x-1)**2 + 1
# plot 函数会把传入的 x, y
# 组成的每一个点依次连接成一个平滑的曲线
# 这样就是我们看到的二次函数的曲线了
plt.plot(x, y)
# axis 函数用来指定坐标系的横轴纵轴的范围
# 这样就表示了 
# 横轴为 [-7, 9]
# 纵轴为 [0, 50]
plt.axis([-7, 9, 0, 50])
# scatter 函数是用来绘制散点图的
# scatter 和 plot 函数不同
# scatter 并不会将每个点依次连接
# 而是直接将它们以点的形式绘制出来
plt.scatter(np.array(all_x), np.array(all_y), color='red')
plt.show()
「图片效果」


通过最终的效果图可以发现，「梯度下降」在一步一步地收敛到二次函数的极小值点，同时也是二次函数的最小值点。仔细看一下，可以发现，就像刚才提到的，随着算法的运行，红色的点和点之间的距离越来越小了，也就是每一次更新的幅度越来越小了。这可不关「学习率」 的事儿，因为在这里用到的学习率是一个固定的值，这只是因为随着接近极值点的过程中「导数的绝对值」越来越小。

同时，我们还可以把 all_x 和 all_y 中最后一个位置存储的值打印出来看一下，如果没错的话，all_x 的最后一个位置的值是接近于 1 的，同时对应的函数值 all_y 最后一个位置的值也应该是接近于 1 的。


3 二元函数的梯度下降
在多元函数中拥有多个自变量，在这里就用二元的函数来举例子好了，二元函数的图像是在三维坐标系中的。下边儿就是一个多元函数的图像例子。在这里纵坐标我使用 「y」 表示，底面的两个坐标分别使用 「x1」 和 「x2」 来表示。不用太纠结底面到底哪个轴是 x1 哪个轴是 x2，这里只是为了说明问题。


「图片来自 维基百科」

通过上面的二元函数凸显可以发现想要让 「y」 的值尽可能地小就要寻找极值点（同样地这个极值点也不一定是最小值点）。梯度是一个由各个自变量的偏导数所组成的一个「向量」。用数学表达式看上去就是下边儿这样的。


算法的过程和上边儿举例子用的二次函数是一样的，为了直观，我就直接使用数学式子的方式表达出来。



同样地，一开始随机初始一个位置，随后让当前位置的坐标值减去学习率乘以当前位置的偏导数。更新自变量 x1 就让 x1 减去学习率乘以 y 对 x1 的偏导数，更新自变量 x2 就让 x2 减去学习率乘以 y 对 x2 的偏导数。

「发挥想象力」

发挥想象力再思考一下，既然梯度是一个向量，那么它就代表了一个方向。 x1 的偏导数的反方向告诉我们在 x1 这个轴上朝向哪个方向变化可以使得函数值 y 减小，x2 的偏导数的反方向告诉我们在 x2 这个轴上朝向哪个方向变化可以使得函数值 y 减小。那么，由偏导数组成的向量就告诉了我们在底面朝向哪个方向走就可以使得函数 y 减小。

在下边儿的图中，我使用红色箭头表示我们当前所在的位置，随后我使用 「黑色」 箭头代表其中一个轴上的坐标朝向哪个方向变化可以使得函数值 「y」 减小，并使用 「黄色」 箭头代表另外一个轴上的坐标朝向哪个方向变化可以使得函数值 「y」 减小。根据平行色变形法则有了 「黑色」 向量和 「黄色」 向量，就可以知道这两个向量最终达到的效果就是 「蓝色」 向量所达到的效果。


最终，对于二元函数梯度下降的理解，就可以理解为求出的梯度的反方向在底面给了我们一个方向，只要朝着这个方向变化底面的坐标就可以使得函数值 「y」 变小。

再看一下刚才的式子，是不是就觉得很亲切了呢。



其实上边儿的式子变成向量的形式，还可以写成下边儿这个样子。


4 多元函数的梯度下降
对于更加一般的形式，比二元更多的自变量的表达式就是下边儿这个样子了。由于超过了二元就已经没有办法画出来了。因为二元函数需要在三维坐标系下，三元函数需要在四维坐标系下，依次类推。


梯度下降法」 的目标是搜索出来一个能够让函数的值尽可能小的位置，所以应该让 
 朝着黑色箭头的方向走，那么怎么完成这个操作呢？接下来的过程我使用伪代码来表示。

在代码中有一个 
 变量，它的专业名词叫 「学习率」。使用数学表达式来表示更新 
 的过程那就是：


表达式的意思就是让 
 减去 
 乘以函数的导数。其中的 
 是为了控制 
 更新的幅度，将 
 设置的小一点，那么每一次更新的幅度就会小一点。这是有用的，在后边儿我会解释 
 的设置会出现什么样的问题。

「初始化：」

# 变量 x 表示当前所在的位置
# 就随机初始在 6 这个位置好了
x = 6
eta = 0.05
「第 1 次更新 x」

# 变量 df 存储当前位置的导数值
df = 2*x-2
x = x - eta*df
# 更新后 x 由 6 变成 
# 6-0.05*10 = 5.5
「第 2 次更新 x」

# 因为 x 的位置发生了变化
# 要重新计算当前位置的导数
df = 2*x-2
x = x - eta*df
# 更新后 x 由 5.5 变成
# 5.5-0.05*9 = 5.05
好啦，接下来只要重复刚才的过程就可以了。那么，把刚才的两个步骤画出来。其中红色箭头指向的点是初始时候的点，「黄色」 箭头指向的点是第 1 次更新后的位置，「蓝色」 箭头指向的点是第 2 次更新后的位置。


只要不停地重复刚才的这个过程，那么最终就会收敛到一个 「局部」 的极值点。那么可以想一下这是为什么？因为随着每一次的更新，曲线都会越来越平缓，相应的导数值也会越来越小，当我们接近极值点的时候导数的值会无限地靠近 「0」。由于导数的绝对值越来越小，那么随后更新的幅度也会越来越小，最终就会停留在极值点的位置了。接下来我将用 Python 来实现这个过程，并让刚才的步骤迭代 1000 次。

「算法初始化」

import matplotlib.pyplot as plt
import numpy as np

# 初始算法开始之前的坐标 
# cur_x 和 cur_y 
cur_x = 6
cur_y = (cur_x-1)**2 + 1
# 设置学习率 eta 为 0.05
eta = 0.05
# 变量 iter 用于存储迭代次数
# 这次我们迭代 1000 次
# 所以给它赋值 1000
iter = 1000
# 变量 cur_df 用于存储
# 当前位置的导数
# 一开始我给它赋值为 None
# 每一轮循环的时候为它更新值
cur_df = None

# all_x 用于存储
# 算法进行时所有点的横坐标
all_x = []
# all_y 用于存储
# 算法进行时所有点的纵坐标
all_y = []

# 把最一开始的坐标存储到
# all_x 和 all_y 中
all_x.append(cur_x)
all_y.append(cur_y)
「迭代 1000 次」

# 循环结束也就意味着算法的结束
for i in range(iter):
    # 每一次迭代之前先计算
    # 当前位置的梯度 cur_df
    # cur 是英文单词 current 
    cur_df = 2*cur_x - 2
    # 更新 cur_x 到下一个位置
    cur_x = cur_x - eta*cur_df
    # 更新下一个 cur_x 对应的 cur_y
    cur_y = (cur_x-1)**2 + 1

    # 其实 cur_y 并没有起到实际的计算作用
    # 在这里计算 cur_y 只是为了将每一次的
    # 点的坐标存储到 all_x 和 all_y 中
    # all_x 存储了二维平面上所有点的横坐标
    # all_y 存储了二维平面上所欲点的纵坐标
    # 使用 list 的 append 方法添加元素
    all_x.append(cur_x)
    all_y.append(cur_y)
「绘图」

# 这里的 x, y 值为了绘制二次函数
# 的那根曲线用的，和算法没有关系
# linspace 将会从区间 [-5, 7] 中
# 等距离分割出 100 个点并返回一个
# np.array 类型的对象给 x
x = np.linspace(-5, 7, 100)
# 计算出 x 中每一个横坐标对应的纵坐标
y = (x-1)**2 + 1
# plot 函数会把传入的 x, y
# 组成的每一个点依次连接成一个平滑的曲线
# 这样就是我们看到的二次函数的曲线了
plt.plot(x, y)
# axis 函数用来指定坐标系的横轴纵轴的范围
# 这样就表示了 
# 横轴为 [-7, 9]
# 纵轴为 [0, 50]
plt.axis([-7, 9, 0, 50])
# scatter 函数是用来绘制散点图的
# scatter 和 plot 函数不同
# scatter 并不会将每个点依次连接
# 而是直接将它们以点的形式绘制出来
plt.scatter(np.array(all_x), np.array(all_y), color='red')
plt.show()
「图片效果」


通过最终的效果图可以发现，「梯度下降」在一步一步地收敛到二次函数的极小值点，同时也是二次函数的最小值点。仔细看一下，可以发现，就像刚才提到的，随着算法的运行，红色的点和点之间的距离越来越小了，也就是每一次更新的幅度越来越小了。这可不关「学习率」 的事儿，因为在这里用到的学习率是一个固定的值，这只是因为随着接近极值点的过程中「导数的绝对值」越来越小。

同时，我们还可以把 all_x 和 all_y 中最后一个位置存储的值打印出来看一下，如果没错的话，all_x 的最后一个位置的值是接近于 1 的，同时对应的函数值 all_y 最后一个位置的值也应该是接近于 1 的。


3 二元函数的梯度下降
在多元函数中拥有多个自变量，在这里就用二元的函数来举例子好了，二元函数的图像是在三维坐标系中的。下边儿就是一个多元函数的图像例子。在这里纵坐标我使用 「y」 表示，底面的两个坐标分别使用 「x1」 和 「x2」 来表示。不用太纠结底面到底哪个轴是 x1 哪个轴是 x2，这里只是为了说明问题。


「图片来自 维基百科」

通过上面的二元函数凸显可以发现想要让 「y」 的值尽可能地小就要寻找极值点（同样地这个极值点也不一定是最小值点）。梯度是一个由各个自变量的偏导数所组成的一个「向量」。用数学表达式看上去就是下边儿这样的。


算法的过程和上边儿举例子用的二次函数是一样的，为了直观，我就直接使用数学式子的方式表达出来。



同样地，一开始随机初始一个位置，随后让当前位置的坐标值减去学习率乘以当前位置的偏导数。更新自变量 x1 就让 x1 减去学习率乘以 y 对 x1 的偏导数，更新自变量 x2 就让 x2 减去学习率乘以 y 对 x2 的偏导数。

「发挥想象力」

发挥想象力再思考一下，既然梯度是一个向量，那么它就代表了一个方向。 x1 的偏导数的反方向告诉我们在 x1 这个轴上朝向哪个方向变化可以使得函数值 y 减小，x2 的偏导数的反方向告诉我们在 x2 这个轴上朝向哪个方向变化可以使得函数值 y 减小。那么，由偏导数组成的向量就告诉了我们在底面朝向哪个方向走就可以使得函数 y 减小。

在下边儿的图中，我使用红色箭头表示我们当前所在的位置，随后我使用 「黑色」 箭头代表其中一个轴上的坐标朝向哪个方向变化可以使得函数值 「y」 减小，并使用 「黄色」 箭头代表另外一个轴上的坐标朝向哪个方向变化可以使得函数值 「y」 减小。根据平行色变形法则有了 「黑色」 向量和 「黄色」 向量，就可以知道这两个向量最终达到的效果就是 「蓝色」 向量所达到的效果。


最终，对于二元函数梯度下降的理解，就可以理解为求出的梯度的反方向在底面给了我们一个方向，只要朝着这个方向变化底面的坐标就可以使得函数值 「y」 变小。

再看一下刚才的式子，是不是就觉得很亲切了呢。



其实上边儿的式子变成向量的形式，还可以写成下边儿这个样子。


4 多元函数的梯度下降
对于更加一般的形式，比二元更多的自变量的表达式就是下边儿这个样子了。由于超过了二元就已经没有办法画出来了。因为二元函数需要在三维坐标系下，三元函数需要在四维坐标系下，依次类推。


梯度下降法」 的目标是搜索出来一个能够让函数的值尽可能小的位置，所以应该让 
 朝着黑色箭头的方向走，那么怎么完成这个操作呢？接下来的过程我使用伪代码来表示。

在代码中有一个 
 变量，它的专业名词叫 「学习率」。使用数学表达式来表示更新 
 的过程那就是：


表达式的意思就是让 
 减去 
 乘以函数的导数。其中的 
 是为了控制 
 更新的幅度，将 
 设置的小一点，那么每一次更新的幅度就会小一点。这是有用的，在后边儿我会解释 
 的设置会出现什么样的问题。

「初始化：」

# 变量 x 表示当前所在的位置
# 就随机初始在 6 这个位置好了
x = 6
eta = 0.05
「第 1 次更新 x」

# 变量 df 存储当前位置的导数值
df = 2*x-2
x = x - eta*df
# 更新后 x 由 6 变成 
# 6-0.05*10 = 5.5
「第 2 次更新 x」

# 因为 x 的位置发生了变化
# 要重新计算当前位置的导数
df = 2*x-2
x = x - eta*df
# 更新后 x 由 5.5 变成
# 5.5-0.05*9 = 5.05
好啦，接下来只要重复刚才的过程就可以了。那么，把刚才的两个步骤画出来。其中红色箭头指向的点是初始时候的点，「黄色」 箭头指向的点是第 1 次更新后的位置，「蓝色」 箭头指向的点是第 2 次更新后的位置。


只要不停地重复刚才的这个过程，那么最终就会收敛到一个 「局部」 的极值点。那么可以想一下这是为什么？因为随着每一次的更新，曲线都会越来越平缓，相应的导数值也会越来越小，当我们接近极值点的时候导数的值会无限地靠近 「0」。由于导数的绝对值越来越小，那么随后更新的幅度也会越来越小，最终就会停留在极值点的位置了。接下来我将用 Python 来实现这个过程，并让刚才的步骤迭代 1000 次。

「算法初始化」

import matplotlib.pyplot as plt
import numpy as np

# 初始算法开始之前的坐标 
# cur_x 和 cur_y 
cur_x = 6
cur_y = (cur_x-1)**2 + 1
# 设置学习率 eta 为 0.05
eta = 0.05
# 变量 iter 用于存储迭代次数
# 这次我们迭代 1000 次
# 所以给它赋值 1000
iter = 1000
# 变量 cur_df 用于存储
# 当前位置的导数
# 一开始我给它赋值为 None
# 每一轮循环的时候为它更新值
cur_df = None

# all_x 用于存储
# 算法进行时所有点的横坐标
all_x = []
# all_y 用于存储
# 算法进行时所有点的纵坐标
all_y = []

# 把最一开始的坐标存储到
# all_x 和 all_y 中
all_x.append(cur_x)
all_y.append(cur_y)
「迭代 1000 次」

# 循环结束也就意味着算法的结束
for i in range(iter):
    # 每一次迭代之前先计算
    # 当前位置的梯度 cur_df
    # cur 是英文单词 current 
    cur_df = 2*cur_x - 2
    # 更新 cur_x 到下一个位置
    cur_x = cur_x - eta*cur_df
    # 更新下一个 cur_x 对应的 cur_y
    cur_y = (cur_x-1)**2 + 1

    # 其实 cur_y 并没有起到实际的计算作用
    # 在这里计算 cur_y 只是为了将每一次的
    # 点的坐标存储到 all_x 和 all_y 中
    # all_x 存储了二维平面上所有点的横坐标
    # all_y 存储了二维平面上所欲点的纵坐标
    # 使用 list 的 append 方法添加元素
    all_x.append(cur_x)
    all_y.append(cur_y)
「绘图」

# 这里的 x, y 值为了绘制二次函数
# 的那根曲线用的，和算法没有关系
# linspace 将会从区间 [-5, 7] 中
# 等距离分割出 100 个点并返回一个
# np.array 类型的对象给 x
x = np.linspace(-5, 7, 100)
# 计算出 x 中每一个横坐标对应的纵坐标
y = (x-1)**2 + 1
# plot 函数会把传入的 x, y
# 组成的每一个点依次连接成一个平滑的曲线
# 这样就是我们看到的二次函数的曲线了
plt.plot(x, y)
# axis 函数用来指定坐标系的横轴纵轴的范围
# 这样就表示了 
# 横轴为 [-7, 9]
# 纵轴为 [0, 50]
plt.axis([-7, 9, 0, 50])
# scatter 函数是用来绘制散点图的
# scatter 和 plot 函数不同
# scatter 并不会将每个点依次连接
# 而是直接将它们以点的形式绘制出来
plt.scatter(np.array(all_x), np.array(all_y), color='red')
plt.show()
「图片效果」


通过最终的效果图可以发现，「梯度下降」在一步一步地收敛到二次函数的极小值点，同时也是二次函数的最小值点。仔细看一下，可以发现，就像刚才提到的，随着算法的运行，红色的点和点之间的距离越来越小了，也就是每一次更新的幅度越来越小了。这可不关「学习率」 的事儿，因为在这里用到的学习率是一个固定的值，这只是因为随着接近极值点的过程中「导数的绝对值」越来越小。

同时，我们还可以把 all_x 和 all_y 中最后一个位置存储的值打印出来看一下，如果没错的话，all_x 的最后一个位置的值是接近于 1 的，同时对应的函数值 all_y 最后一个位置的值也应该是接近于 1 的。


3 二元函数的梯度下降
在多元函数中拥有多个自变量，在这里就用二元的函数来举例子好了，二元函数的图像是在三维坐标系中的。下边儿就是一个多元函数的图像例子。在这里纵坐标我使用 「y」 表示，底面的两个坐标分别使用 「x1」 和 「x2」 来表示。不用太纠结底面到底哪个轴是 x1 哪个轴是 x2，这里只是为了说明问题。


「图片来自 维基百科」

通过上面的二元函数凸显可以发现想要让 「y」 的值尽可能地小就要寻找极值点（同样地这个极值点也不一定是最小值点）。梯度是一个由各个自变量的偏导数所组成的一个「向量」。用数学表达式看上去就是下边儿这样的。


算法的过程和上边儿举例子用的二次函数是一样的，为了直观，我就直接使用数学式子的方式表达出来。



同样地，一开始随机初始一个位置，随后让当前位置的坐标值减去学习率乘以当前位置的偏导数。更新自变量 x1 就让 x1 减去学习率乘以 y 对 x1 的偏导数，更新自变量 x2 就让 x2 减去学习率乘以 y 对 x2 的偏导数。

「发挥想象力」

发挥想象力再思考一下，既然梯度是一个向量，那么它就代表了一个方向。 x1 的偏导数的反方向告诉我们在 x1 这个轴上朝向哪个方向变化可以使得函数值 y 减小，x2 的偏导数的反方向告诉我们在 x2 这个轴上朝向哪个方向变化可以使得函数值 y 减小。那么，由偏导数组成的向量就告诉了我们在底面朝向哪个方向走就可以使得函数 y 减小。

在下边儿的图中，我使用红色箭头表示我们当前所在的位置，随后我使用 「黑色」 箭头代表其中一个轴上的坐标朝向哪个方向变化可以使得函数值 「y」 减小，并使用 「黄色」 箭头代表另外一个轴上的坐标朝向哪个方向变化可以使得函数值 「y」 减小。根据平行色变形法则有了 「黑色」 向量和 「黄色」 向量，就可以知道这两个向量最终达到的效果就是 「蓝色」 向量所达到的效果。


最终，对于二元函数梯度下降的理解，就可以理解为求出的梯度的反方向在底面给了我们一个方向，只要朝着这个方向变化底面的坐标就可以使得函数值 「y」 变小。

再看一下刚才的式子，是不是就觉得很亲切了呢。



其实上边儿的式子变成向量的形式，还可以写成下边儿这个样子。


4 多元函数的梯度下降
对于更加一般的形式，比二元更多的自变量的表达式就是下边儿这个样子了。由于超过了二元就已经没有办法画出来了。因为二元函数需要在三维坐标系下，三元函数需要在四维坐标系下，依次类推。


梯度下降法」 的目标是搜索出来一个能够让函数的值尽可能小的位置，所以应该让 
 朝着黑色箭头的方向走，那么怎么完成这个操作呢？接下来的过程我使用伪代码来表示。

在代码中有一个 
 变量，它的专业名词叫 「学习率」。使用数学表达式来表示更新 
 的过程那就是：


表达式的意思就是让 
 减去 
 乘以函数的导数。其中的 
 是为了控制 
 更新的幅度，将 
 设置的小一点，那么每一次更新的幅度就会小一点。这是有用的，在后边儿我会解释 
 的设置会出现什么样的问题。

「初始化：」

# 变量 x 表示当前所在的位置
# 就随机初始在 6 这个位置好了
x = 6
eta = 0.05
「第 1 次更新 x」

# 变量 df 存储当前位置的导数值
df = 2*x-2
x = x - eta*df
# 更新后 x 由 6 变成 
# 6-0.05*10 = 5.5
「第 2 次更新 x」

# 因为 x 的位置发生了变化
# 要重新计算当前位置的导数
df = 2*x-2
x = x - eta*df
# 更新后 x 由 5.5 变成
# 5.5-0.05*9 = 5.05
好啦，接下来只要重复刚才的过程就可以了。那么，把刚才的两个步骤画出来。其中红色箭头指向的点是初始时候的点，「黄色」 箭头指向的点是第 1 次更新后的位置，「蓝色」 箭头指向的点是第 2 次更新后的位置。


只要不停地重复刚才的这个过程，那么最终就会收敛到一个 「局部」 的极值点。那么可以想一下这是为什么？因为随着每一次的更新，曲线都会越来越平缓，相应的导数值也会越来越小，当我们接近极值点的时候导数的值会无限地靠近 「0」。由于导数的绝对值越来越小，那么随后更新的幅度也会越来越小，最终就会停留在极值点的位置了。接下来我将用 Python 来实现这个过程，并让刚才的步骤迭代 1000 次。

「算法初始化」

import matplotlib.pyplot as plt
import numpy as np

# 初始算法开始之前的坐标 
# cur_x 和 cur_y 
cur_x = 6
cur_y = (cur_x-1)**2 + 1
# 设置学习率 eta 为 0.05
eta = 0.05
# 变量 iter 用于存储迭代次数
# 这次我们迭代 1000 次
# 所以给它赋值 1000
iter = 1000
# 变量 cur_df 用于存储
# 当前位置的导数
# 一开始我给它赋值为 None
# 每一轮循环的时候为它更新值
cur_df = None

# all_x 用于存储
# 算法进行时所有点的横坐标
all_x = []
# all_y 用于存储
# 算法进行时所有点的纵坐标
all_y = []

# 把最一开始的坐标存储到
# all_x 和 all_y 中
all_x.append(cur_x)
all_y.append(cur_y)
「迭代 1000 次」

# 循环结束也就意味着算法的结束
for i in range(iter):
    # 每一次迭代之前先计算
    # 当前位置的梯度 cur_df
    # cur 是英文单词 current 
    cur_df = 2*cur_x - 2
    # 更新 cur_x 到下一个位置
    cur_x = cur_x - eta*cur_df
    # 更新下一个 cur_x 对应的 cur_y
    cur_y = (cur_x-1)**2 + 1

    # 其实 cur_y 并没有起到实际的计算作用
    # 在这里计算 cur_y 只是为了将每一次的
    # 点的坐标存储到 all_x 和 all_y 中
    # all_x 存储了二维平面上所有点的横坐标
    # all_y 存储了二维平面上所欲点的纵坐标
    # 使用 list 的 append 方法添加元素
    all_x.append(cur_x)
    all_y.append(cur_y)
「绘图」

# 这里的 x, y 值为了绘制二次函数
# 的那根曲线用的，和算法没有关系
# linspace 将会从区间 [-5, 7] 中
# 等距离分割出 100 个点并返回一个
# np.array 类型的对象给 x
x = np.linspace(-5, 7, 100)
# 计算出 x 中每一个横坐标对应的纵坐标
y = (x-1)**2 + 1
# plot 函数会把传入的 x, y
# 组成的每一个点依次连接成一个平滑的曲线
# 这样就是我们看到的二次函数的曲线了
plt.plot(x, y)
# axis 函数用来指定坐标系的横轴纵轴的范围
# 这样就表示了 
# 横轴为 [-7, 9]
# 纵轴为 [0, 50]
plt.axis([-7, 9, 0, 50])
# scatter 函数是用来绘制散点图的
# scatter 和 plot 函数不同
# scatter 并不会将每个点依次连接
# 而是直接将它们以点的形式绘制出来
plt.scatter(np.array(all_x), np.array(all_y), color='red')
plt.show()
「图片效果」


通过最终的效果图可以发现，「梯度下降」在一步一步地收敛到二次函数的极小值点，同时也是二次函数的最小值点。仔细看一下，可以发现，就像刚才提到的，随着算法的运行，红色的点和点之间的距离越来越小了，也就是每一次更新的幅度越来越小了。这可不关「学习率」 的事儿，因为在这里用到的学习率是一个固定的值，这只是因为随着接近极值点的过程中「导数的绝对值」越来越小。

同时，我们还可以把 all_x 和 all_y 中最后一个位置存储的值打印出来看一下，如果没错的话，all_x 的最后一个位置的值是接近于 1 的，同时对应的函数值 all_y 最后一个位置的值也应该是接近于 1 的。


3 二元函数的梯度下降
在多元函数中拥有多个自变量，在这里就用二元的函数来举例子好了，二元函数的图像是在三维坐标系中的。下边儿就是一个多元函数的图像例子。在这里纵坐标我使用 「y」 表示，底面的两个坐标分别使用 「x1」 和 「x2」 来表示。不用太纠结底面到底哪个轴是 x1 哪个轴是 x2，这里只是为了说明问题。


「图片来自 维基百科」

通过上面的二元函数凸显可以发现想要让 「y」 的值尽可能地小就要寻找极值点（同样地这个极值点也不一定是最小值点）。梯度是一个由各个自变量的偏导数所组成的一个「向量」。用数学表达式看上去就是下边儿这样的。


算法的过程和上边儿举例子用的二次函数是一样的，为了直观，我就直接使用数学式子的方式表达出来。



同样地，一开始随机初始一个位置，随后让当前位置的坐标值减去学习率乘以当前位置的偏导数。更新自变量 x1 就让 x1 减去学习率乘以 y 对 x1 的偏导数，更新自变量 x2 就让 x2 减去学习率乘以 y 对 x2 的偏导数。

「发挥想象力」

发挥想象力再思考一下，既然梯度是一个向量，那么它就代表了一个方向。 x1 的偏导数的反方向告诉我们在 x1 这个轴上朝向哪个方向变化可以使得函数值 y 减小，x2 的偏导数的反方向告诉我们在 x2 这个轴上朝向哪个方向变化可以使得函数值 y 减小。那么，由偏导数组成的向量就告诉了我们在底面朝向哪个方向走就可以使得函数 y 减小。

在下边儿的图中，我使用红色箭头表示我们当前所在的位置，随后我使用 「黑色」 箭头代表其中一个轴上的坐标朝向哪个方向变化可以使得函数值 「y」 减小，并使用 「黄色」 箭头代表另外一个轴上的坐标朝向哪个方向变化可以使得函数值 「y」 减小。根据平行色变形法则有了 「黑色」 向量和 「黄色」 向量，就可以知道这两个向量最终达到的效果就是 「蓝色」 向量所达到的效果。


最终，对于二元函数梯度下降的理解，就可以理解为求出的梯度的反方向在底面给了我们一个方向，只要朝着这个方向变化底面的坐标就可以使得函数值 「y」 变小。

再看一下刚才的式子，是不是就觉得很亲切了呢。



其实上边儿的式子变成向量的形式，还可以写成下边儿这个样子。


4 多元函数的梯度下降
对于更加一般的形式，比二元更多的自变量的表达式就是下边儿这个样子了。由于超过了二元就已经没有办法画出来了。因为二元函数需要在三维坐标系下，三元函数需要在四维坐标系下，依次类推。


梯度下降法」 的目标是搜索出来一个能够让函数的值尽可能小的位置，所以应该让 
 朝着黑色箭头的方向走，那么怎么完成这个操作呢？接下来的过程我使用伪代码来表示。

在代码中有一个 
 变量，它的专业名词叫 「学习率」。使用数学表达式来表示更新 
 的过程那就是：


表达式的意思就是让 
 减去 
 乘以函数的导数。其中的 
 是为了控制 
 更新的幅度，将 
 设置的小一点，那么每一次更新的幅度就会小一点。这是有用的，在后边儿我会解释 
 的设置会出现什么样的问题。

「初始化：」

# 变量 x 表示当前所在的位置
# 就随机初始在 6 这个位置好了
x = 6
eta = 0.05
「第 1 次更新 x」

# 变量 df 存储当前位置的导数值
df = 2*x-2
x = x - eta*df
# 更新后 x 由 6 变成 
# 6-0.05*10 = 5.5
「第 2 次更新 x」

# 因为 x 的位置发生了变化
# 要重新计算当前位置的导数
df = 2*x-2
x = x - eta*df
# 更新后 x 由 5.5 变成
# 5.5-0.05*9 = 5.05
好啦，接下来只要重复刚才的过程就可以了。那么，把刚才的两个步骤画出来。其中红色箭头指向的点是初始时候的点，「黄色」 箭头指向的点是第 1 次更新后的位置，「蓝色」 箭头指向的点是第 2 次更新后的位置。


只要不停地重复刚才的这个过程，那么最终就会收敛到一个 「局部」 的极值点。那么可以想一下这是为什么？因为随着每一次的更新，曲线都会越来越平缓，相应的导数值也会越来越小，当我们接近极值点的时候导数的值会无限地靠近 「0」。由于导数的绝对值越来越小，那么随后更新的幅度也会越来越小，最终就会停留在极值点的位置了。接下来我将用 Python 来实现这个过程，并让刚才的步骤迭代 1000 次。

「算法初始化」

import matplotlib.pyplot as plt
import numpy as np

# 初始算法开始之前的坐标 
# cur_x 和 cur_y 
cur_x = 6
cur_y = (cur_x-1)**2 + 1
# 设置学习率 eta 为 0.05
eta = 0.05
# 变量 iter 用于存储迭代次数
# 这次我们迭代 1000 次
# 所以给它赋值 1000
iter = 1000
# 变量 cur_df 用于存储
# 当前位置的导数
# 一开始我给它赋值为 None
# 每一轮循环的时候为它更新值
cur_df = None

# all_x 用于存储
# 算法进行时所有点的横坐标
all_x = []
# all_y 用于存储
# 算法进行时所有点的纵坐标
all_y = []

# 把最一开始的坐标存储到
# all_x 和 all_y 中
all_x.append(cur_x)
all_y.append(cur_y)
「迭代 1000 次」

# 循环结束也就意味着算法的结束
for i in range(iter):
    # 每一次迭代之前先计算
    # 当前位置的梯度 cur_df
    # cur 是英文单词 current 
    cur_df = 2*cur_x - 2
    # 更新 cur_x 到下一个位置
    cur_x = cur_x - eta*cur_df
    # 更新下一个 cur_x 对应的 cur_y
    cur_y = (cur_x-1)**2 + 1

    # 其实 cur_y 并没有起到实际的计算作用
    # 在这里计算 cur_y 只是为了将每一次的
    # 点的坐标存储到 all_x 和 all_y 中
    # all_x 存储了二维平面上所有点的横坐标
    # all_y 存储了二维平面上所欲点的纵坐标
    # 使用 list 的 append 方法添加元素
    all_x.append(cur_x)
    all_y.append(cur_y)
「绘图」

# 这里的 x, y 值为了绘制二次函数
# 的那根曲线用的，和算法没有关系
# linspace 将会从区间 [-5, 7] 中
# 等距离分割出 100 个点并返回一个
# np.array 类型的对象给 x
x = np.linspace(-5, 7, 100)
# 计算出 x 中每一个横坐标对应的纵坐标
y = (x-1)**2 + 1
# plot 函数会把传入的 x, y
# 组成的每一个点依次连接成一个平滑的曲线
# 这样就是我们看到的二次函数的曲线了
plt.plot(x, y)
# axis 函数用来指定坐标系的横轴纵轴的范围
# 这样就表示了 
# 横轴为 [-7, 9]
# 纵轴为 [0, 50]
plt.axis([-7, 9, 0, 50])
# scatter 函数是用来绘制散点图的
# scatter 和 plot 函数不同
# scatter 并不会将每个点依次连接
# 而是直接将它们以点的形式绘制出来
plt.scatter(np.array(all_x), np.array(all_y), color='red')
plt.show()
「图片效果」


通过最终的效果图可以发现，「梯度下降」在一步一步地收敛到二次函数的极小值点，同时也是二次函数的最小值点。仔细看一下，可以发现，就像刚才提到的，随着算法的运行，红色的点和点之间的距离越来越小了，也就是每一次更新的幅度越来越小了。这可不关「学习率」 的事儿，因为在这里用到的学习率是一个固定的值，这只是因为随着接近极值点的过程中「导数的绝对值」越来越小。

同时，我们还可以把 all_x 和 all_y 中最后一个位置存储的值打印出来看一下，如果没错的话，all_x 的最后一个位置的值是接近于 1 的，同时对应的函数值 all_y 最后一个位置的值也应该是接近于 1 的。


3 二元函数的梯度下降
在多元函数中拥有多个自变量，在这里就用二元的函数来举例子好了，二元函数的图像是在三维坐标系中的。下边儿就是一个多元函数的图像例子。在这里纵坐标我使用 「y」 表示，底面的两个坐标分别使用 「x1」 和 「x2」 来表示。不用太纠结底面到底哪个轴是 x1 哪个轴是 x2，这里只是为了说明问题。


「图片来自 维基百科」

通过上面的二元函数凸显可以发现想要让 「y」 的值尽可能地小就要寻找极值点（同样地这个极值点也不一定是最小值点）。梯度是一个由各个自变量的偏导数所组成的一个「向量」。用数学表达式看上去就是下边儿这样的。


算法的过程和上边儿举例子用的二次函数是一样的，为了直观，我就直接使用数学式子的方式表达出来。



同样地，一开始随机初始一个位置，随后让当前位置的坐标值减去学习率乘以当前位置的偏导数。更新自变量 x1 就让 x1 减去学习率乘以 y 对 x1 的偏导数，更新自变量 x2 就让 x2 减去学习率乘以 y 对 x2 的偏导数。

「发挥想象力」

发挥想象力再思考一下，既然梯度是一个向量，那么它就代表了一个方向。 x1 的偏导数的反方向告诉我们在 x1 这个轴上朝向哪个方向变化可以使得函数值 y 减小，x2 的偏导数的反方向告诉我们在 x2 这个轴上朝向哪个方向变化可以使得函数值 y 减小。那么，由偏导数组成的向量就告诉了我们在底面朝向哪个方向走就可以使得函数 y 减小。

在下边儿的图中，我使用红色箭头表示我们当前所在的位置，随后我使用 「黑色」 箭头代表其中一个轴上的坐标朝向哪个方向变化可以使得函数值 「y」 减小，并使用 「黄色」 箭头代表另外一个轴上的坐标朝向哪个方向变化可以使得函数值 「y」 减小。根据平行色变形法则有了 「黑色」 向量和 「黄色」 向量，就可以知道这两个向量最终达到的效果就是 「蓝色」 向量所达到的效果。


最终，对于二元函数梯度下降的理解，就可以理解为求出的梯度的反方向在底面给了我们一个方向，只要朝着这个方向变化底面的坐标就可以使得函数值 「y」 变小。

再看一下刚才的式子，是不是就觉得很亲切了呢。



其实上边儿的式子变成向量的形式，还可以写成下边儿这个样子。


4 多元函数的梯度下降
对于更加一般的形式，比二元更多的自变量的表达式就是下边儿这个样子了。由于超过了二元就已经没有办法画出来了。因为二元函数需要在三维坐标系下，三元函数需要在四维坐标系下，依次类推。


梯度下降法」 的目标是搜索出来一个能够让函数的值尽可能小的位置，所以应该让 
 朝着黑色箭头的方向走，那么怎么完成这个操作呢？接下来的过程我使用伪代码来表示。

在代码中有一个 
 变量，它的专业名词叫 「学习率」。使用数学表达式来表示更新 
 的过程那就是：


表达式的意思就是让 
 减去 
 乘以函数的导数。其中的 
 是为了控制 
 更新的幅度，将 
 设置的小一点，那么每一次更新的幅度就会小一点。这是有用的，在后边儿我会解释 
 的设置会出现什么样的问题。

「初始化：」

# 变量 x 表示当前所在的位置
# 就随机初始在 6 这个位置好了
x = 6
eta = 0.05
「第 1 次更新 x」

# 变量 df 存储当前位置的导数值
df = 2*x-2
x = x - eta*df
# 更新后 x 由 6 变成 
# 6-0.05*10 = 5.5
「第 2 次更新 x」

# 因为 x 的位置发生了变化
# 要重新计算当前位置的导数
df = 2*x-2
x = x - eta*df
# 更新后 x 由 5.5 变成
# 5.5-0.05*9 = 5.05
好啦，接下来只要重复刚才的过程就可以了。那么，把刚才的两个步骤画出来。其中红色箭头指向的点是初始时候的点，「黄色」 箭头指向的点是第 1 次更新后的位置，「蓝色」 箭头指向的点是第 2 次更新后的位置。


只要不停地重复刚才的这个过程，那么最终就会收敛到一个 「局部」 的极值点。那么可以想一下这是为什么？因为随着每一次的更新，曲线都会越来越平缓，相应的导数值也会越来越小，当我们接近极值点的时候导数的值会无限地靠近 「0」。由于导数的绝对值越来越小，那么随后更新的幅度也会越来越小，最终就会停留在极值点的位置了。接下来我将用 Python 来实现这个过程，并让刚才的步骤迭代 1000 次。

「算法初始化」

import matplotlib.pyplot as plt
import numpy as np

# 初始算法开始之前的坐标 
# cur_x 和 cur_y 
cur_x = 6
cur_y = (cur_x-1)**2 + 1
# 设置学习率 eta 为 0.05
eta = 0.05
# 变量 iter 用于存储迭代次数
# 这次我们迭代 1000 次
# 所以给它赋值 1000
iter = 1000
# 变量 cur_df 用于存储
# 当前位置的导数
# 一开始我给它赋值为 None
# 每一轮循环的时候为它更新值
cur_df = None

# all_x 用于存储
# 算法进行时所有点的横坐标
all_x = []
# all_y 用于存储
# 算法进行时所有点的纵坐标
all_y = []

# 把最一开始的坐标存储到
# all_x 和 all_y 中
all_x.append(cur_x)
all_y.append(cur_y)
「迭代 1000 次」

# 循环结束也就意味着算法的结束
for i in range(iter):
    # 每一次迭代之前先计算
    # 当前位置的梯度 cur_df
    # cur 是英文单词 current 
    cur_df = 2*cur_x - 2
    # 更新 cur_x 到下一个位置
    cur_x = cur_x - eta*cur_df
    # 更新下一个 cur_x 对应的 cur_y
    cur_y = (cur_x-1)**2 + 1

    # 其实 cur_y 并没有起到实际的计算作用
    # 在这里计算 cur_y 只是为了将每一次的
    # 点的坐标存储到 all_x 和 all_y 中
    # all_x 存储了二维平面上所有点的横坐标
    # all_y 存储了二维平面上所欲点的纵坐标
    # 使用 list 的 append 方法添加元素
    all_x.append(cur_x)
    all_y.append(cur_y)
「绘图」

# 这里的 x, y 值为了绘制二次函数
# 的那根曲线用的，和算法没有关系
# linspace 将会从区间 [-5, 7] 中
# 等距离分割出 100 个点并返回一个
# np.array 类型的对象给 x
x = np.linspace(-5, 7, 100)
# 计算出 x 中每一个横坐标对应的纵坐标
y = (x-1)**2 + 1
# plot 函数会把传入的 x, y
# 组成的每一个点依次连接成一个平滑的曲线
# 这样就是我们看到的二次函数的曲线了
plt.plot(x, y)
# axis 函数用来指定坐标系的横轴纵轴的范围
# 这样就表示了 
# 横轴为 [-7, 9]
# 纵轴为 [0, 50]
plt.axis([-7, 9, 0, 50])
# scatter 函数是用来绘制散点图的
# scatter 和 plot 函数不同
# scatter 并不会将每个点依次连接
# 而是直接将它们以点的形式绘制出来
plt.scatter(np.array(all_x), np.array(all_y), color='red')
plt.show()
「图片效果」


通过最终的效果图可以发现，「梯度下降」在一步一步地收敛到二次函数的极小值点，同时也是二次函数的最小值点。仔细看一下，可以发现，就像刚才提到的，随着算法的运行，红色的点和点之间的距离越来越小了，也就是每一次更新的幅度越来越小了。这可不关「学习率」 的事儿，因为在这里用到的学习率是一个固定的值，这只是因为随着接近极值点的过程中「导数的绝对值」越来越小。

同时，我们还可以把 all_x 和 all_y 中最后一个位置存储的值打印出来看一下，如果没错的话，all_x 的最后一个位置的值是接近于 1 的，同时对应的函数值 all_y 最后一个位置的值也应该是接近于 1 的。


3 二元函数的梯度下降
在多元函数中拥有多个自变量，在这里就用二元的函数来举例子好了，二元函数的图像是在三维坐标系中的。下边儿就是一个多元函数的图像例子。在这里纵坐标我使用 「y」 表示，底面的两个坐标分别使用 「x1」 和 「x2」 来表示。不用太纠结底面到底哪个轴是 x1 哪个轴是 x2，这里只是为了说明问题。


「图片来自 维基百科」

通过上面的二元函数凸显可以发现想要让 「y」 的值尽可能地小就要寻找极值点（同样地这个极值点也不一定是最小值点）。梯度是一个由各个自变量的偏导数所组成的一个「向量」。用数学表达式看上去就是下边儿这样的。


算法的过程和上边儿举例子用的二次函数是一样的，为了直观，我就直接使用数学式子的方式表达出来。



同样地，一开始随机初始一个位置，随后让当前位置的坐标值减去学习率乘以当前位置的偏导数。更新自变量 x1 就让 x1 减去学习率乘以 y 对 x1 的偏导数，更新自变量 x2 就让 x2 减去学习率乘以 y 对 x2 的偏导数。

「发挥想象力」

发挥想象力再思考一下，既然梯度是一个向量，那么它就代表了一个方向。 x1 的偏导数的反方向告诉我们在 x1 这个轴上朝向哪个方向变化可以使得函数值 y 减小，x2 的偏导数的反方向告诉我们在 x2 这个轴上朝向哪个方向变化可以使得函数值 y 减小。那么，由偏导数组成的向量就告诉了我们在底面朝向哪个方向走就可以使得函数 y 减小。

在下边儿的图中，我使用红色箭头表示我们当前所在的位置，随后我使用 「黑色」 箭头代表其中一个轴上的坐标朝向哪个方向变化可以使得函数值 「y」 减小，并使用 「黄色」 箭头代表另外一个轴上的坐标朝向哪个方向变化可以使得函数值 「y」 减小。根据平行色变形法则有了 「黑色」 向量和 「黄色」 向量，就可以知道这两个向量最终达到的效果就是 「蓝色」 向量所达到的效果。


最终，对于二元函数梯度下降的理解，就可以理解为求出的梯度的反方向在底面给了我们一个方向，只要朝着这个方向变化底面的坐标就可以使得函数值 「y」 变小。

再看一下刚才的式子，是不是就觉得很亲切了呢。



其实上边儿的式子变成向量的形式，还可以写成下边儿这个样子。


4 多元函数的梯度下降
对于更加一般的形式，比二元更多的自变量的表达式就是下边儿这个样子了。由于超过了二元就已经没有办法画出来了。因为二元函数需要在三维坐标系下，三元函数需要在四维坐标系下，依次类推。


梯度下降法」 的目标是搜索出来一个能够让函数的值尽可能小的位置，所以应该让 
 朝着黑色箭头的方向走，那么怎么完成这个操作呢？接下来的过程我使用伪代码来表示。

在代码中有一个 
 变量，它的专业名词叫 「学习率」。使用数学表达式来表示更新 
 的过程那就是：


表达式的意思就是让 
 减去 
 乘以函数的导数。其中的 
 是为了控制 
 更新的幅度，将 
 设置的小一点，那么每一次更新的幅度就会小一点。这是有用的，在后边儿我会解释 
 的设置会出现什么样的问题。

「初始化：」

# 变量 x 表示当前所在的位置
# 就随机初始在 6 这个位置好了
x = 6
eta = 0.05
「第 1 次更新 x」

# 变量 df 存储当前位置的导数值
df = 2*x-2
x = x - eta*df
# 更新后 x 由 6 变成 
# 6-0.05*10 = 5.5
「第 2 次更新 x」

# 因为 x 的位置发生了变化
# 要重新计算当前位置的导数
df = 2*x-2
x = x - eta*df
# 更新后 x 由 5.5 变成
# 5.5-0.05*9 = 5.05
好啦，接下来只要重复刚才的过程就可以了。那么，把刚才的两个步骤画出来。其中红色箭头指向的点是初始时候的点，「黄色」 箭头指向的点是第 1 次更新后的位置，「蓝色」 箭头指向的点是第 2 次更新后的位置。


只要不停地重复刚才的这个过程，那么最终就会收敛到一个 「局部」 的极值点。那么可以想一下这是为什么？因为随着每一次的更新，曲线都会越来越平缓，相应的导数值也会越来越小，当我们接近极值点的时候导数的值会无限地靠近 「0」。由于导数的绝对值越来越小，那么随后更新的幅度也会越来越小，最终就会停留在极值点的位置了。接下来我将用 Python 来实现这个过程，并让刚才的步骤迭代 1000 次。

「算法初始化」

import matplotlib.pyplot as plt
import numpy as np

# 初始算法开始之前的坐标 
# cur_x 和 cur_y 
cur_x = 6
cur_y = (cur_x-1)**2 + 1
# 设置学习率 eta 为 0.05
eta = 0.05
# 变量 iter 用于存储迭代次数
# 这次我们迭代 1000 次
# 所以给它赋值 1000
iter = 1000
# 变量 cur_df 用于存储
# 当前位置的导数
# 一开始我给它赋值为 None
# 每一轮循环的时候为它更新值
cur_df = None

# all_x 用于存储
# 算法进行时所有点的横坐标
all_x = []
# all_y 用于存储
# 算法进行时所有点的纵坐标
all_y = []

# 把最一开始的坐标存储到
# all_x 和 all_y 中
all_x.append(cur_x)
all_y.append(cur_y)
「迭代 1000 次」

# 循环结束也就意味着算法的结束
for i in range(iter):
    # 每一次迭代之前先计算
    # 当前位置的梯度 cur_df
    # cur 是英文单词 current 
    cur_df = 2*cur_x - 2
    # 更新 cur_x 到下一个位置
    cur_x = cur_x - eta*cur_df
    # 更新下一个 cur_x 对应的 cur_y
    cur_y = (cur_x-1)**2 + 1

    # 其实 cur_y 并没有起到实际的计算作用
    # 在这里计算 cur_y 只是为了将每一次的
    # 点的坐标存储到 all_x 和 all_y 中
    # all_x 存储了二维平面上所有点的横坐标
    # all_y 存储了二维平面上所欲点的纵坐标
    # 使用 list 的 append 方法添加元素
    all_x.append(cur_x)
    all_y.append(cur_y)
「绘图」

# 这里的 x, y 值为了绘制二次函数
# 的那根曲线用的，和算法没有关系
# linspace 将会从区间 [-5, 7] 中
# 等距离分割出 100 个点并返回一个
# np.array 类型的对象给 x
x = np.linspace(-5, 7, 100)
# 计算出 x 中每一个横坐标对应的纵坐标
y = (x-1)**2 + 1
# plot 函数会把传入的 x, y
# 组成的每一个点依次连接成一个平滑的曲线
# 这样就是我们看到的二次函数的曲线了
plt.plot(x, y)
# axis 函数用来指定坐标系的横轴纵轴的范围
# 这样就表示了 
# 横轴为 [-7, 9]
# 纵轴为 [0, 50]
plt.axis([-7, 9, 0, 50])
# scatter 函数是用来绘制散点图的
# scatter 和 plot 函数不同
# scatter 并不会将每个点依次连接
# 而是直接将它们以点的形式绘制出来
plt.scatter(np.array(all_x), np.array(all_y), color='red')
plt.show()
「图片效果」


通过最终的效果图可以发现，「梯度下降」在一步一步地收敛到二次函数的极小值点，同时也是二次函数的最小值点。仔细看一下，可以发现，就像刚才提到的，随着算法的运行，红色的点和点之间的距离越来越小了，也就是每一次更新的幅度越来越小了。这可不关「学习率」 的事儿，因为在这里用到的学习率是一个固定的值，这只是因为随着接近极值点的过程中「导数的绝对值」越来越小。

同时，我们还可以把 all_x 和 all_y 中最后一个位置存储的值打印出来看一下，如果没错的话，all_x 的最后一个位置的值是接近于 1 的，同时对应的函数值 all_y 最后一个位置的值也应该是接近于 1 的。


3 二元函数的梯度下降
在多元函数中拥有多个自变量，在这里就用二元的函数来举例子好了，二元函数的图像是在三维坐标系中的。下边儿就是一个多元函数的图像例子。在这里纵坐标我使用 「y」 表示，底面的两个坐标分别使用 「x1」 和 「x2」 来表示。不用太纠结底面到底哪个轴是 x1 哪个轴是 x2，这里只是为了说明问题。


「图片来自 维基百科」

通过上面的二元函数凸显可以发现想要让 「y」 的值尽可能地小就要寻找极值点（同样地这个极值点也不一定是最小值点）。梯度是一个由各个自变量的偏导数所组成的一个「向量」。用数学表达式看上去就是下边儿这样的。


算法的过程和上边儿举例子用的二次函数是一样的，为了直观，我就直接使用数学式子的方式表达出来。



同样地，一开始随机初始一个位置，随后让当前位置的坐标值减去学习率乘以当前位置的偏导数。更新自变量 x1 就让 x1 减去学习率乘以 y 对 x1 的偏导数，更新自变量 x2 就让 x2 减去学习率乘以 y 对 x2 的偏导数。

「发挥想象力」

发挥想象力再思考一下，既然梯度是一个向量，那么它就代表了一个方向。 x1 的偏导数的反方向告诉我们在 x1 这个轴上朝向哪个方向变化可以使得函数值 y 减小，x2 的偏导数的反方向告诉我们在 x2 这个轴上朝向哪个方向变化可以使得函数值 y 减小。那么，由偏导数组成的向量就告诉了我们在底面朝向哪个方向走就可以使得函数 y 减小。

在下边儿的图中，我使用红色箭头表示我们当前所在的位置，随后我使用 「黑色」 箭头代表其中一个轴上的坐标朝向哪个方向变化可以使得函数值 「y」 减小，并使用 「黄色」 箭头代表另外一个轴上的坐标朝向哪个方向变化可以使得函数值 「y」 减小。根据平行色变形法则有了 「黑色」 向量和 「黄色」 向量，就可以知道这两个向量最终达到的效果就是 「蓝色」 向量所达到的效果。


最终，对于二元函数梯度下降的理解，就可以理解为求出的梯度的反方向在底面给了我们一个方向，只要朝着这个方向变化底面的坐标就可以使得函数值 「y」 变小。

再看一下刚才的式子，是不是就觉得很亲切了呢。



其实上边儿的式子变成向量的形式，还可以写成下边儿这个样子。


4 多元函数的梯度下降
对于更加一般的形式，比二元更多的自变量的表达式就是下边儿这个样子了。由于超过了二元就已经没有办法画出来了。因为二元函数需要在三维坐标系下，三元函数需要在四维坐标系下，依次类推。


梯度下降法」 的目标是搜索出来一个能够让函数的值尽可能小的位置，所以应该让 
 朝着黑色箭头的方向走，那么怎么完成这个操作呢？接下来的过程我使用伪代码来表示。

在代码中有一个 
 变量，它的专业名词叫 「学习率」。使用数学表达式来表示更新 
 的过程那就是：


表达式的意思就是让 
 减去 
 乘以函数的导数。其中的 
 是为了控制 
 更新的幅度，将 
 设置的小一点，那么每一次更新的幅度就会小一点。这是有用的，在后边儿我会解释 
 的设置会出现什么样的问题。

「初始化：」

# 变量 x 表示当前所在的位置
# 就随机初始在 6 这个位置好了
x = 6
eta = 0.05
「第 1 次更新 x」

# 变量 df 存储当前位置的导数值
df = 2*x-2
x = x - eta*df
# 更新后 x 由 6 变成 
# 6-0.05*10 = 5.5
「第 2 次更新 x」

# 因为 x 的位置发生了变化
# 要重新计算当前位置的导数
df = 2*x-2
x = x - eta*df
# 更新后 x 由 5.5 变成
# 5.5-0.05*9 = 5.05
好啦，接下来只要重复刚才的过程就可以了。那么，把刚才的两个步骤画出来。其中红色箭头指向的点是初始时候的点，「黄色」 箭头指向的点是第 1 次更新后的位置，「蓝色」 箭头指向的点是第 2 次更新后的位置。


只要不停地重复刚才的这个过程，那么最终就会收敛到一个 「局部」 的极值点。那么可以想一下这是为什么？因为随着每一次的更新，曲线都会越来越平缓，相应的导数值也会越来越小，当我们接近极值点的时候导数的值会无限地靠近 「0」。由于导数的绝对值越来越小，那么随后更新的幅度也会越来越小，最终就会停留在极值点的位置了。接下来我将用 Python 来实现这个过程，并让刚才的步骤迭代 1000 次。

「算法初始化」

import matplotlib.pyplot as plt
import numpy as np

# 初始算法开始之前的坐标 
# cur_x 和 cur_y 
cur_x = 6
cur_y = (cur_x-1)**2 + 1
# 设置学习率 eta 为 0.05
eta = 0.05
# 变量 iter 用于存储迭代次数
# 这次我们迭代 1000 次
# 所以给它赋值 1000
iter = 1000
# 变量 cur_df 用于存储
# 当前位置的导数
# 一开始我给它赋值为 None
# 每一轮循环的时候为它更新值
cur_df = None

# all_x 用于存储
# 算法进行时所有点的横坐标
all_x = []
# all_y 用于存储
# 算法进行时所有点的纵坐标
all_y = []

# 把最一开始的坐标存储到
# all_x 和 all_y 中
all_x.append(cur_x)
all_y.append(cur_y)
「迭代 1000 次」

# 循环结束也就意味着算法的结束
for i in range(iter):
    # 每一次迭代之前先计算
    # 当前位置的梯度 cur_df
    # cur 是英文单词 current 
    cur_df = 2*cur_x - 2
    # 更新 cur_x 到下一个位置
    cur_x = cur_x - eta*cur_df
    # 更新下一个 cur_x 对应的 cur_y
    cur_y = (cur_x-1)**2 + 1

    # 其实 cur_y 并没有起到实际的计算作用
    # 在这里计算 cur_y 只是为了将每一次的
    # 点的坐标存储到 all_x 和 all_y 中
    # all_x 存储了二维平面上所有点的横坐标
    # all_y 存储了二维平面上所欲点的纵坐标
    # 使用 list 的 append 方法添加元素
    all_x.append(cur_x)
    all_y.append(cur_y)
「绘图」

# 这里的 x, y 值为了绘制二次函数
# 的那根曲线用的，和算法没有关系
# linspace 将会从区间 [-5, 7] 中
# 等距离分割出 100 个点并返回一个
# np.array 类型的对象给 x
x = np.linspace(-5, 7, 100)
# 计算出 x 中每一个横坐标对应的纵坐标
y = (x-1)**2 + 1
# plot 函数会把传入的 x, y
# 组成的每一个点依次连接成一个平滑的曲线
# 这样就是我们看到的二次函数的曲线了
plt.plot(x, y)
# axis 函数用来指定坐标系的横轴纵轴的范围
# 这样就表示了 
# 横轴为 [-7, 9]
# 纵轴为 [0, 50]
plt.axis([-7, 9, 0, 50])
# scatter 函数是用来绘制散点图的
# scatter 和 plot 函数不同
# scatter 并不会将每个点依次连接
# 而是直接将它们以点的形式绘制出来
plt.scatter(np.array(all_x), np.array(all_y), color='red')
plt.show()
「图片效果」


通过最终的效果图可以发现，「梯度下降」在一步一步地收敛到二次函数的极小值点，同时也是二次函数的最小值点。仔细看一下，可以发现，就像刚才提到的，随着算法的运行，红色的点和点之间的距离越来越小了，也就是每一次更新的幅度越来越小了。这可不关「学习率」 的事儿，因为在这里用到的学习率是一个固定的值，这只是因为随着接近极值点的过程中「导数的绝对值」越来越小。

同时，我们还可以把 all_x 和 all_y 中最后一个位置存储的值打印出来看一下，如果没错的话，all_x 的最后一个位置的值是接近于 1 的，同时对应的函数值 all_y 最后一个位置的值也应该是接近于 1 的。


3 二元函数的梯度下降
在多元函数中拥有多个自变量，在这里就用二元的函数来举例子好了，二元函数的图像是在三维坐标系中的。下边儿就是一个多元函数的图像例子。在这里纵坐标我使用 「y」 表示，底面的两个坐标分别使用 「x1」 和 「x2」 来表示。不用太纠结底面到底哪个轴是 x1 哪个轴是 x2，这里只是为了说明问题。


「图片来自 维基百科」

通过上面的二元函数凸显可以发现想要让 「y」 的值尽可能地小就要寻找极值点（同样地这个极值点也不一定是最小值点）。梯度是一个由各个自变量的偏导数所组成的一个「向量」。用数学表达式看上去就是下边儿这样的。


算法的过程和上边儿举例子用的二次函数是一样的，为了直观，我就直接使用数学式子的方式表达出来。



同样地，一开始随机初始一个位置，随后让当前位置的坐标值减去学习率乘以当前位置的偏导数。更新自变量 x1 就让 x1 减去学习率乘以 y 对 x1 的偏导数，更新自变量 x2 就让 x2 减去学习率乘以 y 对 x2 的偏导数。

「发挥想象力」

发挥想象力再思考一下，既然梯度是一个向量，那么它就代表了一个方向。 x1 的偏导数的反方向告诉我们在 x1 这个轴上朝向哪个方向变化可以使得函数值 y 减小，x2 的偏导数的反方向告诉我们在 x2 这个轴上朝向哪个方向变化可以使得函数值 y 减小。那么，由偏导数组成的向量就告诉了我们在底面朝向哪个方向走就可以使得函数 y 减小。

在下边儿的图中，我使用红色箭头表示我们当前所在的位置，随后我使用 「黑色」 箭头代表其中一个轴上的坐标朝向哪个方向变化可以使得函数值 「y」 减小，并使用 「黄色」 箭头代表另外一个轴上的坐标朝向哪个方向变化可以使得函数值 「y」 减小。根据平行色变形法则有了 「黑色」 向量和 「黄色」 向量，就可以知道这两个向量最终达到的效果就是 「蓝色」 向量所达到的效果。


最终，对于二元函数梯度下降的理解，就可以理解为求出的梯度的反方向在底面给了我们一个方向，只要朝着这个方向变化底面的坐标就可以使得函数值 「y」 变小。

再看一下刚才的式子，是不是就觉得很亲切了呢。



其实上边儿的式子变成向量的形式，还可以写成下边儿这个样子。


4 多元函数的梯度下降
对于更加一般的形式，比二元更多的自变量的表达式就是下边儿这个样子了。由于超过了二元就已经没有办法画出来了。因为二元函数需要在三维坐标系下，三元函数需要在四维坐标系下，依次类推。


梯度下降法」 的目标是搜索出来一个能够让函数的值尽可能小的位置，所以应该让 
 朝着黑色箭头的方向走，那么怎么完成这个操作呢？接下来的过程我使用伪代码来表示。

在代码中有一个 
 变量，它的专业名词叫 「学习率」。使用数学表达式来表示更新 
 的过程那就是：


表达式的意思就是让 
 减去 
 乘以函数的导数。其中的 
 是为了控制 
 更新的幅度，将 
 设置的小一点，那么每一次更新的幅度就会小一点。这是有用的，在后边儿我会解释 
 的设置会出现什么样的问题。

「初始化：」

# 变量 x 表示当前所在的位置
# 就随机初始在 6 这个位置好了
x = 6
eta = 0.05
「第 1 次更新 x」

# 变量 df 存储当前位置的导数值
df = 2*x-2
x = x - eta*df
# 更新后 x 由 6 变成 
# 6-0.05*10 = 5.5
「第 2 次更新 x」

# 因为 x 的位置发生了变化
# 要重新计算当前位置的导数
df = 2*x-2
x = x - eta*df
# 更新后 x 由 5.5 变成
# 5.5-0.05*9 = 5.05
好啦，接下来只要重复刚才的过程就可以了。那么，把刚才的两个步骤画出来。其中红色箭头指向的点是初始时候的点，「黄色」 箭头指向的点是第 1 次更新后的位置，「蓝色」 箭头指向的点是第 2 次更新后的位置。


只要不停地重复刚才的这个过程，那么最终就会收敛到一个 「局部」 的极值点。那么可以想一下这是为什么？因为随着每一次的更新，曲线都会越来越平缓，相应的导数值也会越来越小，当我们接近极值点的时候导数的值会无限地靠近 「0」。由于导数的绝对值越来越小，那么随后更新的幅度也会越来越小，最终就会停留在极值点的位置了。接下来我将用 Python 来实现这个过程，并让刚才的步骤迭代 1000 次。

「算法初始化」

import matplotlib.pyplot as plt
import numpy as np

# 初始算法开始之前的坐标 
# cur_x 和 cur_y 
cur_x = 6
cur_y = (cur_x-1)**2 + 1
# 设置学习率 eta 为 0.05
eta = 0.05
# 变量 iter 用于存储迭代次数
# 这次我们迭代 1000 次
# 所以给它赋值 1000
iter = 1000
# 变量 cur_df 用于存储
# 当前位置的导数
# 一开始我给它赋值为 None
# 每一轮循环的时候为它更新值
cur_df = None

# all_x 用于存储
# 算法进行时所有点的横坐标
all_x = []
# all_y 用于存储
# 算法进行时所有点的纵坐标
all_y = []

# 把最一开始的坐标存储到
# all_x 和 all_y 中
all_x.append(cur_x)
all_y.append(cur_y)
「迭代 1000 次」

# 循环结束也就意味着算法的结束
for i in range(iter):
    # 每一次迭代之前先计算
    # 当前位置的梯度 cur_df
    # cur 是英文单词 current 
    cur_df = 2*cur_x - 2
    # 更新 cur_x 到下一个位置
    cur_x = cur_x - eta*cur_df
    # 更新下一个 cur_x 对应的 cur_y
    cur_y = (cur_x-1)**2 + 1

    # 其实 cur_y 并没有起到实际的计算作用
    # 在这里计算 cur_y 只是为了将每一次的
    # 点的坐标存储到 all_x 和 all_y 中
    # all_x 存储了二维平面上所有点的横坐标
    # all_y 存储了二维平面上所欲点的纵坐标
    # 使用 list 的 append 方法添加元素
    all_x.append(cur_x)
    all_y.append(cur_y)
「绘图」

# 这里的 x, y 值为了绘制二次函数
# 的那根曲线用的，和算法没有关系
# linspace 将会从区间 [-5, 7] 中
# 等距离分割出 100 个点并返回一个
# np.array 类型的对象给 x
x = np.linspace(-5, 7, 100)
# 计算出 x 中每一个横坐标对应的纵坐标
y = (x-1)**2 + 1
# plot 函数会把传入的 x, y
# 组成的每一个点依次连接成一个平滑的曲线
# 这样就是我们看到的二次函数的曲线了
plt.plot(x, y)
# axis 函数用来指定坐标系的横轴纵轴的范围
# 这样就表示了 
# 横轴为 [-7, 9]
# 纵轴为 [0, 50]
plt.axis([-7, 9, 0, 50])
# scatter 函数是用来绘制散点图的
# scatter 和 plot 函数不同
# scatter 并不会将每个点依次连接
# 而是直接将它们以点的形式绘制出来
plt.scatter(np.array(all_x), np.array(all_y), color='red')
plt.show()
「图片效果」


通过最终的效果图可以发现，「梯度下降」在一步一步地收敛到二次函数的极小值点，同时也是二次函数的最小值点。仔细看一下，可以发现，就像刚才提到的，随着算法的运行，红色的点和点之间的距离越来越小了，也就是每一次更新的幅度越来越小了。这可不关「学习率」 的事儿，因为在这里用到的学习率是一个固定的值，这只是因为随着接近极值点的过程中「导数的绝对值」越来越小。

同时，我们还可以把 all_x 和 all_y 中最后一个位置存储的值打印出来看一下，如果没错的话，all_x 的最后一个位置的值是接近于 1 的，同时对应的函数值 all_y 最后一个位置的值也应该是接近于 1 的。


3 二元函数的梯度下降
在多元函数中拥有多个自变量，在这里就用二元的函数来举例子好了，二元函数的图像是在三维坐标系中的。下边儿就是一个多元函数的图像例子。在这里纵坐标我使用 「y」 表示，底面的两个坐标分别使用 「x1」 和 「x2」 来表示。不用太纠结底面到底哪个轴是 x1 哪个轴是 x2，这里只是为了说明问题。


「图片来自 维基百科」

通过上面的二元函数凸显可以发现想要让 「y」 的值尽可能地小就要寻找极值点（同样地这个极值点也不一定是最小值点）。梯度是一个由各个自变量的偏导数所组成的一个「向量」。用数学表达式看上去就是下边儿这样的。


算法的过程和上边儿举例子用的二次函数是一样的，为了直观，我就直接使用数学式子的方式表达出来。



同样地，一开始随机初始一个位置，随后让当前位置的坐标值减去学习率乘以当前位置的偏导数。更新自变量 x1 就让 x1 减去学习率乘以 y 对 x1 的偏导数，更新自变量 x2 就让 x2 减去学习率乘以 y 对 x2 的偏导数。

「发挥想象力」

发挥想象力再思考一下，既然梯度是一个向量，那么它就代表了一个方向。 x1 的偏导数的反方向告诉我们在 x1 这个轴上朝向哪个方向变化可以使得函数值 y 减小，x2 的偏导数的反方向告诉我们在 x2 这个轴上朝向哪个方向变化可以使得函数值 y 减小。那么，由偏导数组成的向量就告诉了我们在底面朝向哪个方向走就可以使得函数 y 减小。

在下边儿的图中，我使用红色箭头表示我们当前所在的位置，随后我使用 「黑色」 箭头代表其中一个轴上的坐标朝向哪个方向变化可以使得函数值 「y」 减小，并使用 「黄色」 箭头代表另外一个轴上的坐标朝向哪个方向变化可以使得函数值 「y」 减小。根据平行色变形法则有了 「黑色」 向量和 「黄色」 向量，就可以知道这两个向量最终达到的效果就是 「蓝色」 向量所达到的效果。


最终，对于二元函数梯度下降的理解，就可以理解为求出的梯度的反方向在底面给了我们一个方向，只要朝着这个方向变化底面的坐标就可以使得函数值 「y」 变小。

再看一下刚才的式子，是不是就觉得很亲切了呢。



其实上边儿的式子变成向量的形式，还可以写成下边儿这个样子。


4 多元函数的梯度下降
对于更加一般的形式，比二元更多的自变量的表达式就是下边儿这个样子了。由于超过了二元就已经没有办法画出来了。因为二元函数需要在三维坐标系下，三元函数需要在四维坐标系下，依次类推。


梯度下降法」 的目标是搜索出来一个能够让函数的值尽可能小的位置，所以应该让 
 朝着黑色箭头的方向走，那么怎么完成这个操作呢？接下来的过程我使用伪代码来表示。

在代码中有一个 
 变量，它的专业名词叫 「学习率」。使用数学表达式来表示更新 
 的过程那就是：


表达式的意思就是让 
 减去 
 乘以函数的导数。其中的 
 是为了控制 
 更新的幅度，将 
 设置的小一点，那么每一次更新的幅度就会小一点。这是有用的，在后边儿我会解释 
 的设置会出现什么样的问题。

「初始化：」

# 变量 x 表示当前所在的位置
# 就随机初始在 6 这个位置好了
x = 6
eta = 0.05
「第 1 次更新 x」

# 变量 df 存储当前位置的导数值
df = 2*x-2
x = x - eta*df
# 更新后 x 由 6 变成 
# 6-0.05*10 = 5.5
「第 2 次更新 x」

# 因为 x 的位置发生了变化
# 要重新计算当前位置的导数
df = 2*x-2
x = x - eta*df
# 更新后 x 由 5.5 变成
# 5.5-0.05*9 = 5.05
好啦，接下来只要重复刚才的过程就可以了。那么，把刚才的两个步骤画出来。其中红色箭头指向的点是初始时候的点，「黄色」 箭头指向的点是第 1 次更新后的位置，「蓝色」 箭头指向的点是第 2 次更新后的位置。


只要不停地重复刚才的这个过程，那么最终就会收敛到一个 「局部」 的极值点。那么可以想一下这是为什么？因为随着每一次的更新，曲线都会越来越平缓，相应的导数值也会越来越小，当我们接近极值点的时候导数的值会无限地靠近 「0」。由于导数的绝对值越来越小，那么随后更新的幅度也会越来越小，最终就会停留在极值点的位置了。接下来我将用 Python 来实现这个过程，并让刚才的步骤迭代 1000 次。

「算法初始化」

import matplotlib.pyplot as plt
import numpy as np

# 初始算法开始之前的坐标 
# cur_x 和 cur_y 
cur_x = 6
cur_y = (cur_x-1)**2 + 1
# 设置学习率 eta 为 0.05
eta = 0.05
# 变量 iter 用于存储迭代次数
# 这次我们迭代 1000 次
# 所以给它赋值 1000
iter = 1000
# 变量 cur_df 用于存储
# 当前位置的导数
# 一开始我给它赋值为 None
# 每一轮循环的时候为它更新值
cur_df = None

# all_x 用于存储
# 算法进行时所有点的横坐标
all_x = []
# all_y 用于存储
# 算法进行时所有点的纵坐标
all_y = []

# 把最一开始的坐标存储到
# all_x 和 all_y 中
all_x.append(cur_x)
all_y.append(cur_y)
「迭代 1000 次」

# 循环结束也就意味着算法的结束
for i in range(iter):
    # 每一次迭代之前先计算
    # 当前位置的梯度 cur_df
    # cur 是英文单词 current 
    cur_df = 2*cur_x - 2
    # 更新 cur_x 到下一个位置
    cur_x = cur_x - eta*cur_df
    # 更新下一个 cur_x 对应的 cur_y
    cur_y = (cur_x-1)**2 + 1

    # 其实 cur_y 并没有起到实际的计算作用
    # 在这里计算 cur_y 只是为了将每一次的
    # 点的坐标存储到 all_x 和 all_y 中
    # all_x 存储了二维平面上所有点的横坐标
    # all_y 存储了二维平面上所欲点的纵坐标
    # 使用 list 的 append 方法添加元素
    all_x.append(cur_x)
    all_y.append(cur_y)
「绘图」

# 这里的 x, y 值为了绘制二次函数
# 的那根曲线用的，和算法没有关系
# linspace 将会从区间 [-5, 7] 中
# 等距离分割出 100 个点并返回一个
# np.array 类型的对象给 x
x = np.linspace(-5, 7, 100)
# 计算出 x 中每一个横坐标对应的纵坐标
y = (x-1)**2 + 1
# plot 函数会把传入的 x, y
# 组成的每一个点依次连接成一个平滑的曲线
# 这样就是我们看到的二次函数的曲线了
plt.plot(x, y)
# axis 函数用来指定坐标系的横轴纵轴的范围
# 这样就表示了 
# 横轴为 [-7, 9]
# 纵轴为 [0, 50]
plt.axis([-7, 9, 0, 50])
# scatter 函数是用来绘制散点图的
# scatter 和 plot 函数不同
# scatter 并不会将每个点依次连接
# 而是直接将它们以点的形式绘制出来
plt.scatter(np.array(all_x), np.array(all_y), color='red')
plt.show()
「图片效果」


通过最终的效果图可以发现，「梯度下降」在一步一步地收敛到二次函数的极小值点，同时也是二次函数的最小值点。仔细看一下，可以发现，就像刚才提到的，随着算法的运行，红色的点和点之间的距离越来越小了，也就是每一次更新的幅度越来越小了。这可不关「学习率」 的事儿，因为在这里用到的学习率是一个固定的值，这只是因为随着接近极值点的过程中「导数的绝对值」越来越小。

同时，我们还可以把 all_x 和 all_y 中最后一个位置存储的值打印出来看一下，如果没错的话，all_x 的最后一个位置的值是接近于 1 的，同时对应的函数值 all_y 最后一个位置的值也应该是接近于 1 的。


3 二元函数的梯度下降
在多元函数中拥有多个自变量，在这里就用二元的函数来举例子好了，二元函数的图像是在三维坐标系中的。下边儿就是一个多元函数的图像例子。在这里纵坐标我使用 「y」 表示，底面的两个坐标分别使用 「x1」 和 「x2」 来表示。不用太纠结底面到底哪个轴是 x1 哪个轴是 x2，这里只是为了说明问题。


「图片来自 维基百科」

通过上面的二元函数凸显可以发现想要让 「y」 的值尽可能地小就要寻找极值点（同样地这个极值点也不一定是最小值点）。梯度是一个由各个自变量的偏导数所组成的一个「向量」。用数学表达式看上去就是下边儿这样的。


算法的过程和上边儿举例子用的二次函数是一样的，为了直观，我就直接使用数学式子的方式表达出来。



同样地，一开始随机初始一个位置，随后让当前位置的坐标值减去学习率乘以当前位置的偏导数。更新自变量 x1 就让 x1 减去学习率乘以 y 对 x1 的偏导数，更新自变量 x2 就让 x2 减去学习率乘以 y 对 x2 的偏导数。

「发挥想象力」

发挥想象力再思考一下，既然梯度是一个向量，那么它就代表了一个方向。 x1 的偏导数的反方向告诉我们在 x1 这个轴上朝向哪个方向变化可以使得函数值 y 减小，x2 的偏导数的反方向告诉我们在 x2 这个轴上朝向哪个方向变化可以使得函数值 y 减小。那么，由偏导数组成的向量就告诉了我们在底面朝向哪个方向走就可以使得函数 y 减小。

在下边儿的图中，我使用红色箭头表示我们当前所在的位置，随后我使用 「黑色」 箭头代表其中一个轴上的坐标朝向哪个方向变化可以使得函数值 「y」 减小，并使用 「黄色」 箭头代表另外一个轴上的坐标朝向哪个方向变化可以使得函数值 「y」 减小。根据平行色变形法则有了 「黑色」 向量和 「黄色」 向量，就可以知道这两个向量最终达到的效果就是 「蓝色」 向量所达到的效果。


最终，对于二元函数梯度下降的理解，就可以理解为求出的梯度的反方向在底面给了我们一个方向，只要朝着这个方向变化底面的坐标就可以使得函数值 「y」 变小。

再看一下刚才的式子，是不是就觉得很亲切了呢。



其实上边儿的式子变成向量的形式，还可以写成下边儿这个样子。


4 多元函数的梯度下降
对于更加一般的形式，比二元更多的自变量的表达式就是下边儿这个样子了。由于超过了二元就已经没有办法画出来了。因为二元函数需要在三维坐标系下，三元函数需要在四维坐标系下，依次类推。


梯度下降法」 的目标是搜索出来一个能够让函数的值尽可能小的位置，所以应该让 
 朝着黑色箭头的方向走，那么怎么完成这个操作呢？接下来的过程我使用伪代码来表示。

在代码中有一个 
 变量，它的专业名词叫 「学习率」。使用数学表达式来表示更新 
 的过程那就是：


表达式的意思就是让 
 减去 
 乘以函数的导数。其中的 
 是为了控制 
 更新的幅度，将 
 设置的小一点，那么每一次更新的幅度就会小一点。这是有用的，在后边儿我会解释 
 的设置会出现什么样的问题。

「初始化：」

# 变量 x 表示当前所在的位置
# 就随机初始在 6 这个位置好了
x = 6
eta = 0.05
「第 1 次更新 x」

# 变量 df 存储当前位置的导数值
df = 2*x-2
x = x - eta*df
# 更新后 x 由 6 变成 
# 6-0.05*10 = 5.5
「第 2 次更新 x」

# 因为 x 的位置发生了变化
# 要重新计算当前位置的导数
df = 2*x-2
x = x - eta*df
# 更新后 x 由 5.5 变成
# 5.5-0.05*9 = 5.05
好啦，接下来只要重复刚才的过程就可以了。那么，把刚才的两个步骤画出来。其中红色箭头指向的点是初始时候的点，「黄色」 箭头指向的点是第 1 次更新后的位置，「蓝色」 箭头指向的点是第 2 次更新后的位置。


只要不停地重复刚才的这个过程，那么最终就会收敛到一个 「局部」 的极值点。那么可以想一下这是为什么？因为随着每一次的更新，曲线都会越来越平缓，相应的导数值也会越来越小，当我们接近极值点的时候导数的值会无限地靠近 「0」。由于导数的绝对值越来越小，那么随后更新的幅度也会越来越小，最终就会停留在极值点的位置了。接下来我将用 Python 来实现这个过程，并让刚才的步骤迭代 1000 次。

「算法初始化」

import matplotlib.pyplot as plt
import numpy as np

# 初始算法开始之前的坐标 
# cur_x 和 cur_y 
cur_x = 6
cur_y = (cur_x-1)**2 + 1
# 设置学习率 eta 为 0.05
eta = 0.05
# 变量 iter 用于存储迭代次数
# 这次我们迭代 1000 次
# 所以给它赋值 1000
iter = 1000
# 变量 cur_df 用于存储
# 当前位置的导数
# 一开始我给它赋值为 None
# 每一轮循环的时候为它更新值
cur_df = None

# all_x 用于存储
# 算法进行时所有点的横坐标
all_x = []
# all_y 用于存储
# 算法进行时所有点的纵坐标
all_y = []

# 把最一开始的坐标存储到
# all_x 和 all_y 中
all_x.append(cur_x)
all_y.append(cur_y)
「迭代 1000 次」

# 循环结束也就意味着算法的结束
for i in range(iter):
    # 每一次迭代之前先计算
    # 当前位置的梯度 cur_df
    # cur 是英文单词 current 
    cur_df = 2*cur_x - 2
    # 更新 cur_x 到下一个位置
    cur_x = cur_x - eta*cur_df
    # 更新下一个 cur_x 对应的 cur_y
    cur_y = (cur_x-1)**2 + 1

    # 其实 cur_y 并没有起到实际的计算作用
    # 在这里计算 cur_y 只是为了将每一次的
    # 点的坐标存储到 all_x 和 all_y 中
    # all_x 存储了二维平面上所有点的横坐标
    # all_y 存储了二维平面上所欲点的纵坐标
    # 使用 list 的 append 方法添加元素
    all_x.append(cur_x)
    all_y.append(cur_y)
「绘图」

# 这里的 x, y 值为了绘制二次函数
# 的那根曲线用的，和算法没有关系
# linspace 将会从区间 [-5, 7] 中
# 等距离分割出 100 个点并返回一个
# np.array 类型的对象给 x
x = np.linspace(-5, 7, 100)
# 计算出 x 中每一个横坐标对应的纵坐标
y = (x-1)**2 + 1
# plot 函数会把传入的 x, y
# 组成的每一个点依次连接成一个平滑的曲线
# 这样就是我们看到的二次函数的曲线了
plt.plot(x, y)
# axis 函数用来指定坐标系的横轴纵轴的范围
# 这样就表示了 
# 横轴为 [-7, 9]
# 纵轴为 [0, 50]
plt.axis([-7, 9, 0, 50])
# scatter 函数是用来绘制散点图的
# scatter 和 plot 函数不同
# scatter 并不会将每个点依次连接
# 而是直接将它们以点的形式绘制出来
plt.scatter(np.array(all_x), np.array(all_y), color='red')
plt.show()
「图片效果」


通过最终的效果图可以发现，「梯度下降」在一步一步地收敛到二次函数的极小值点，同时也是二次函数的最小值点。仔细看一下，可以发现，就像刚才提到的，随着算法的运行，红色的点和点之间的距离越来越小了，也就是每一次更新的幅度越来越小了。这可不关「学习率」 的事儿，因为在这里用到的学习率是一个固定的值，这只是因为随着接近极值点的过程中「导数的绝对值」越来越小。

同时，我们还可以把 all_x 和 all_y 中最后一个位置存储的值打印出来看一下，如果没错的话，all_x 的最后一个位置的值是接近于 1 的，同时对应的函数值 all_y 最后一个位置的值也应该是接近于 1 的。


3 二元函数的梯度下降
在多元函数中拥有多个自变量，在这里就用二元的函数来举例子好了，二元函数的图像是在三维坐标系中的。下边儿就是一个多元函数的图像例子。在这里纵坐标我使用 「y」 表示，底面的两个坐标分别使用 「x1」 和 「x2」 来表示。不用太纠结底面到底哪个轴是 x1 哪个轴是 x2，这里只是为了说明问题。


「图片来自 维基百科」

通过上面的二元函数凸显可以发现想要让 「y」 的值尽可能地小就要寻找极值点（同样地这个极值点也不一定是最小值点）。梯度是一个由各个自变量的偏导数所组成的一个「向量」。用数学表达式看上去就是下边儿这样的。


算法的过程和上边儿举例子用的二次函数是一样的，为了直观，我就直接使用数学式子的方式表达出来。



同样地，一开始随机初始一个位置，随后让当前位置的坐标值减去学习率乘以当前位置的偏导数。更新自变量 x1 就让 x1 减去学习率乘以 y 对 x1 的偏导数，更新自变量 x2 就让 x2 减去学习率乘以 y 对 x2 的偏导数。

「发挥想象力」

发挥想象力再思考一下，既然梯度是一个向量，那么它就代表了一个方向。 x1 的偏导数的反方向告诉我们在 x1 这个轴上朝向哪个方向变化可以使得函数值 y 减小，x2 的偏导数的反方向告诉我们在 x2 这个轴上朝向哪个方向变化可以使得函数值 y 减小。那么，由偏导数组成的向量就告诉了我们在底面朝向哪个方向走就可以使得函数 y 减小。

在下边儿的图中，我使用红色箭头表示我们当前所在的位置，随后我使用 「黑色」 箭头代表其中一个轴上的坐标朝向哪个方向变化可以使得函数值 「y」 减小，并使用 「黄色」 箭头代表另外一个轴上的坐标朝向哪个方向变化可以使得函数值 「y」 减小。根据平行色变形法则有了 「黑色」 向量和 「黄色」 向量，就可以知道这两个向量最终达到的效果就是 「蓝色」 向量所达到的效果。


最终，对于二元函数梯度下降的理解，就可以理解为求出的梯度的反方向在底面给了我们一个方向，只要朝着这个方向变化底面的坐标就可以使得函数值 「y」 变小。

再看一下刚才的式子，是不是就觉得很亲切了呢。



其实上边儿的式子变成向量的形式，还可以写成下边儿这个样子。


4 多元函数的梯度下降
对于更加一般的形式，比二元更多的自变量的表达式就是下边儿这个样子了。由于超过了二元就已经没有办法画出来了。因为二元函数需要在三维坐标系下，三元函数需要在四维坐标系下，依次类推。


